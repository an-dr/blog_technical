<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>hackaday on technical_</title><link>https://blog.agramakov.me/tags/hackaday/</link><description>Recent content in hackaday on technical_</description><generator>Hugo</generator><language/><lastBuildDate>Mon, 22 Mar 2021 20:52:00 +0200</lastBuildDate><atom:link href="https://blog.agramakov.me/tags/hackaday/index.xml" rel="self" type="application/rss+xml"/><item><title>"On the way to AliveOS" or "making of a robot with emotions is harder than I thought"</title><link>https://blog.agramakov.me/posts/2021/03-22-on-the-way-to-aliveos-or-making-of-a-robot-with-emotions-is-harder-than-i-thought/</link><pubDate>Mon, 22 Mar 2021 20:52:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/03-22-on-the-way-to-aliveos-or-making-of-a-robot-with-emotions-is-harder-than-i-thought/</guid><description><p>So, indeed, the more I develop the software for Zakhar, the more complicated it goes. So, first:</p><p><strong>Contributions and collaborations are welcome!</strong></p><p>If you want to participate, write to me and we will find what you can do in the project.</p><p>Second, feature branches got toooooo huge, so I&rsquo;ll use the workflow with the develop branch (<a href="https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow" target="_blank">Gitflow</a>) to accumulate less polished features and see some rhythmic development. Currently, I&rsquo;m actively (sometimes reluctantly) working on the integration of my<a href="https://github.com/an-dr/r_giskard_EmotionCore" target="_blank">EmotionCore</a> to the Zakhar&rsquo;s ROS system. So now, the last results are available in the<code>develop</code> branch:</p><p><a href="https://github.com/an-dr/zakharos_core/tree/develop" target="_blank">https://github.com/an-dr/zakharos_core/tree/develop</a></p><p>Third, to simplify the development of the robot&rsquo;s core software I would like to separate really new features from the implementation. So, Zakhar will be a demonstration platform and the mascot of the project&rsquo;s core. The core I&rsquo;d like to name the AliveOS, since implementation of the alive-like behavior is the main goal of the project. I am not sure entirely what the thing AliveOS will be. It is not an operating system so far, rather a framework, but I like the name :).</p><p>Fourth and the last. Usually, I write such posts after some accomplishment. That&rsquo;s true and today. I just merged the<a href="https://github.com/an-dr/zakharos_core/commit/0dedc96056a619cac51373bb9638a029522e7753" target="_blank">feature/emotion_core</a> branch to the develop one (see above). It is not a ready to use feature, but the core is working, getting affected by sensors and exchanging data with other nodes. It doesn&rsquo;t affect the behavior for now. For this I need to make some huge structural changes. The draft bellow (changes are in black and white)</p><p><img class="img-zoomable" src="2587901616447418757.png" alt=""/></p><p>Next steps:</p><ol><li>Implementing of the new architecture changes</li><li>Separation of the core packages into the one called AliveOS</li><li>Moving AliveOS to the separated repository and including it as a submodule (the repo already exists:<a href="https://github.com/an-dr/aliveos" target="_blank">https://github.com/an-dr/aliveos</a>)</li></ol><p>Thank you for reading! Stay tuned and participate!</p></description></item><item><title>EmotionCore - 1.0.0</title><link>https://blog.agramakov.me/posts/2021/02-23-emotioncore-1-0-0/</link><pubDate>Tue, 23 Feb 2021 19:59:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/02-23-emotioncore-1-0-0/</guid><description><p>First release!</p><p>The aim of this library is to implement an emotion model that could be used by other applications implementing behavior modifications (emotions) based on changing input data.</p><p>The Core has sets of<strong><strong><strong><strong>parameters</strong></strong></strong></strong>,<strong><strong><strong><strong>states</strong></strong></strong></strong> and<strong><strong><strong><strong>descriptors</strong></strong></strong></strong>:</p><p><strong><strong><strong><strong>Parameters</strong></strong></strong></strong> define the<strong><strong><strong><strong>state</strong></strong></strong></strong> of the core. Each state has a unique name (happy, sad, etc.)<strong><strong><strong><strong>Descriptors</strong></strong></strong></strong> specify the effect caused by input data to the parameters. The user can use either the state or the set of parameters as output data. Those effects can be:</p><ul><li>depending on sensor data</li><li>depending on time (temporary impacts)</li></ul><p>It is a cross-platform library used in the Zakhar project. You can use this library to implement sophisticated behavior of any of your device. Contribution and ideas are welcome!</p><p>Home page:<a href="https://github.com/an-dr/r_giskard_EmotionCore" target="_blank">r_giskard*_*EmotionCore: Implementing a model of emotions</a></p><p><img class="img-zoomable" src="534801614110287280.png" alt=""/></p></description></item><item><title>Updated draft of the ROS-node network with the Emotional Core</title><link>https://blog.agramakov.me/posts/2021/01-28-updated-draft-of-the-ros-node-network-with-the-emotional-core/</link><pubDate>Thu, 28 Jan 2021 15:25:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-28-updated-draft-of-the-ros-node-network-with-the-emotional-core/</guid><description><p>Working on the Emotion Core update it became clear to me that<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/188030-draft-of-the-updated-ros-node-network-with-the-emotional-core" target="_blank">placing responsibility of emotional analysis to Ego-like nodes</a> (a Consciousness, Reflexes, and Instincts) is a wrong approach. It leads to the situation where the developer of the application should specify several types of behavior themself, which is too complicated for development.</p><p>I want to implement another approach when concepts themselves contain information about how they should be modified based on a set of the emotion parameters. For example:</p><ol><li>An Ego-like node sends the concept<code>move</code> with a modifier<code>left</code>:</li></ol><div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-1"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-1">1</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-2"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-2">2</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-3"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-3">3</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-4"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-4">4</a></span></code></pre></td><td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"concept"</span>:<span style="color:#f1fa8c">"move"</span>,</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"modifier"</span>: [<span style="color:#f1fa8c">"left"</span>]</span></span><span style="display:flex;"><span>}</span></span></code></pre></td></tr></table></div></div><ol start="2"><li><p>The concept<code>move</code> contains a descriptor with information that: if adrenaline is lower than 5 - add a modifier<code>slower</code></p></li><li><p>The Concept-to-command interpreter update the concept to:</p></li></ol><div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-1"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-1">1</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-2"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-2">2</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-3"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-3">3</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-4"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-4">4</a></span></code></pre></td><td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"concept"</span>:<span style="color:#f1fa8c">"move"</span>,</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"modifier"</span>: [<span style="color:#f1fa8c">"left"</span>,<span style="color:#f1fa8c">"slower"</span>]</span></span><span style="display:flex;"><span>}</span></span></code></pre></td></tr></table></div></div><ol start="4"><li>According the concept descriptors the Concept-to-command interpreter sends commands to the device Moving Platform:</li></ol><ul><li><a href="https://github.com/an-dr/zakhar_platform/blob/master/software/main/src/controlcallback.cpp#L36" target="_blank">Set speed to 2</a> (3 is maximum for the device)</li><li><a href="https://github.com/an-dr/zakhar_platform/blob/master/software/main/src/controlcallback.cpp#L29" target="_blank">Move left</a></li><li><a href="https://github.com/an-dr/zakhar_platform/blob/master/software/main/src/controlcallback.cpp#L37" target="_blank">Set speed to 3</a> (default value)</li></ul><p>In addition, I updated the diagram itself by structuring it and adding notes to make the entire system easier to understand. Here is the diagram:</p><p><img class="img-zoomable" src="1018361611848693962.png" alt=""/>
Now I will implement the structure above in the<a href="https://github.com/an-dr/zakharos_core/tree/feature/emotion_core" target="_blank">emotion_core</a> branch of the zakharos_core repository. The feature is getting closer to implementation. More updates soon.</p><p>Links:</p><p><a href="https://github.com/an-dr/zakharos_core/tree/feature/emotion_core" target="_blank">https://github.com/an-dr/zakharos_core/tree/feature/emotion_core</a></p><p><a href="https://github.com/an-dr/r_giskard_EmotionCore" target="_blank">https://github.com/an-dr/r_giskard_EmotionCore</a></p></description></item><item><title>Draft of the updated ROS-node network with the Emotional Core</title><link>https://blog.agramakov.me/posts/2021/01-12-draft-of-the-updated-ros-node-network-with-the-emotional-core/</link><pubDate>Tue, 12 Jan 2021 12:38:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-12-draft-of-the-updated-ros-node-network-with-the-emotional-core/</guid><description><p>Complexity of the networks is getting increased in a non-linear manner with any new type of node added, so documentation of the development is becoming more crucial than ever. So, I&rsquo;ve decided to, firstly redraw the network diagram to make it easier to read and contain more useful information. Then I&rsquo;ve spent some time on how the<a href="https://github.com/an-dr/r_giskard/tree/master/emotional_core" target="_blank">Emotional Core</a> should interact with other nodes.</p><p>Initially, I thought that naming each condition based on a set of emotional parameters is a clever idea. With that approach, we would only send the name of the emotion to the main program and that emotion would affect the robot behavior. But it leads to the situation when the robot has a set of discrete states, in other words, I would develop a pretty sophisticated state machine - and it is way far from how the animals behave.</p><p>Analyzing my feelings, I also cannot say that the name of emotions defines my behavior, I would say that it is rather an uninterrupted specter of states. So, it would be that the researches of<a href="https://en.wikipedia.org/wiki/Carroll_Izard" target="_blank">Carroll Izard</a> and his colleagues in distinguishing human emotions I&rsquo;ve read a lot last months are not applicable to my project. As well as the part of the Emotional Core responsible for changing emotions depending on a set of emotional parameters. A bit sad about spent time, but it is the development process.</p><p>So now I have a draft of the structure I will use integrating the Emotional Core into the mind of Zakhar.</p><p><img class="img-zoomable" src="4854811610453051265.png" alt=""/></p></description></item><item><title>Update of the ROS-network</title><link>https://blog.agramakov.me/posts/2021/01-10-update-of-the-ros-network/</link><pubDate>Sun, 10 Jan 2021 21:32:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-10-update-of-the-ros-network/</guid><description><p>Hi! I merged a huge update for<a href="https://github.com/an-dr/zakharos_core" target="_blank">zakharos_core</a> - the main part of the Zakhar project.</p><p>The repository is a<a href="http://wiki.ros.org/" target="_blank">Robot Operating System</a> network where the main application consists of ego-like nodes: the consciousness (the main application) and instincts (interruptions). Each of them operates with concepts in the manner as our mind works.</p><p>As I already said, my aim is to make a robot that behaves like an animal, and hence having more understandable for the user behavior.</p><p>After this update the next step is to integrate my already developed<a href="https://github.com/an-dr/r_giskard/tree/master/emotional_core" target="_blank">emotional core</a> , which is basically an attempt to recreate the endocrine system of alive organisms.</p><p>Returning to the update. Currently the robot uses three ego-like nodes:</p><ul><li>node-consciousness: the Small researcher - robot is moving by circles.</li><li>instinct: Bird Panic - analyzing light changing patterns to recognizing a single moving fast shadow, then put the robot in the panic state, when it is trying to find a darker (safer) place.</li><li>instinct: Avoid Close Objects - every object closer then 5 cm should be avoided. Object in front leads to moving backward. Objects at sides activate the algorithm: move back, turn on 60 degrees.</li></ul><p>The ROS-network is shown on the picture bellow. Also have a look at the robot in action on a video. Thanks!</p><p><img class="img-zoomable" src="4782381610313730599.png" alt=""/></p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/syaosnG6t_o" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>Hardware structure</title><link>https://blog.agramakov.me/posts/2021/01-09-hardware-structure/</link><pubDate>Sat, 09 Jan 2021 16:15:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-09-hardware-structure/</guid><description><p>Just added illustrations describing hardware used in the project to the repository:</p><p><a href="https://github.com/an-dr/zakhar/tree/ec7c4e69af0f5b44aa3dde2fd7766416ac783668" target="_blank">https://github.com/an-dr/zakhar/tree/ec7c4e69af0f5b44aa3dde2fd7766416ac783668</a></p><p>Here are the pictures:</p><p><img class="img-zoomable" src="4001381610208793411.png" alt=""/></p><p>And detailed for each device:</p><p><img class="img-zoomable" src="4956611610208844010.png" alt=""/><img class="img-zoomable" src="9870381610208816820.png" alt=""/><img class="img-zoomable" src="4438441610208825001.png" alt=""/><img class="img-zoomable" src="1942821610208834869.png" alt=""/></p></description></item><item><title>First test of the second instinct for Zakhar</title><link>https://blog.agramakov.me/posts/2021/01-08-first-test-of-the-second-instinct-for-zakhar/</link><pubDate>Fri, 08 Jan 2021 23:13:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-08-first-test-of-the-second-instinct-for-zakhar/</guid><description><p>Testing of the mind of Zakhar with two &ldquo;instincts&rdquo;:</p><ol><li><p>If a bird-like shadow above him, he expresses anxious and hides</p></li><li><p>If any obstacle in front - move back</p></li></ol><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/UETOz4RXgqY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>New test site!</title><link>https://blog.agramakov.me/posts/2021/01-07-new-test-site/</link><pubDate>Thu, 07 Jan 2021 20:44:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-07-new-test-site/</guid><description><p>Just finished a new site for tests and demonstrations!</p><p><img class="img-zoomable" src="9955191610052220783.jpeg" alt=""/></p><p>New place, new hardware, more updates soon!</p></description></item><item><title>Three Ultrasound Sensors</title><link>https://blog.agramakov.me/posts/2020/12-17-three-ultrasound-sensors/</link><pubDate>Thu, 17 Dec 2020 20:02:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/12-17-three-ultrasound-sensors/</guid><description><p>Unfortunately, I broke my ultrasound sensor during resoldering of connectors. Taking advantage of necessity of buying something, I&rsquo;ve bought three new ones!</p><p><img class="img-zoomable" src="6016331608235294592.jpg" alt=""/>
Updated code with a new ultrasound sensor driver:</p><p><a href="https://github.com/an-dr/zakhar_sensors/tree/c4e7a5e5c91acbbc1efdaaa4122e46428793c973" target="_blank">https://github.com/an-dr/zakhar_sensors/tree/c4e7a5e5c91acbbc1efdaaa4122e46428793c973</a></p></description></item><item><title>Sensor Platform with STM32</title><link>https://blog.agramakov.me/posts/2020/12-07-sensor-platform-with-stm32/</link><pubDate>Mon, 07 Dec 2020 20:10:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/12-07-sensor-platform-with-stm32/</guid><description><p>Done! Now sensors data is collecting by a powerful STM32 MCU with FreeRTOS. This will help adding even more sensors. For now, there are two:</p><ul><li><a href="https://www.sparkfun.com/products/15569" target="_blank">Ultrasonic Distance Sensor - HC-SR04</a></li><li><a href="https://arduinomodules.info/ky-018-photoresistor-module/" target="_blank">KY-018 Photoresistor Module</a></li></ul><p>Here is a video of the update in action:</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/L2-5uH89-tY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Links:</p><ul><li><a href="https://github.com/an-dr/zakhar_sensors/tree/ebcc7f652904122da11c9facf0f1f92af8f6b9fe" target="_blank">Updated source code with STM32 support</a></li><li><a href="https://github.com/an-dr/SharedVirtualRegisters" target="_blank">Repository of my SharedVirtualRegisters library that I used in the update</a> - it is thread-safe and supports FreeRTOS, but can work without any OS as well</li><li><a href="https://github.com/an-dr/log.cx" target="_blank">Repository of the logging library</a> - it is a fork of the<a href="https://github.com/rxi/log.c" target="_blank">log.c library</a> by rxi.</li></ul><p>Photos bellow.</p><p><img class="img-zoomable" src="3319311607372116905.jpg" alt=""/><img class="img-zoomable" src="5859161607372162135.jpg" alt=""/><img class="img-zoomable" src="6079091607371932650.jpg" alt=""/><img class="img-zoomable" src="9371511607372071739.jpg" alt=""/></p></description></item><item><title>Calm prototyping evening</title><link>https://blog.agramakov.me/posts/2020/11-16-calm-prototyping-evening/</link><pubDate>Mon, 16 Nov 2020 20:03:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/11-16-calm-prototyping-evening/</guid><description><p>It&rsquo;s a good night to move the sensor platform to a new MCU and add some sensors to demonstrate working of the<a href="https://github.com/an-dr/r_giskard/tree/master/emotional_core" target="_blank">emotional core</a> (<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">roadmap</a>).</p><p>PR for the update (WIP):<a href="https://github.com/an-dr/zakhar_sensors/pull/4" target="_blank">https://github.com/an-dr/zakhar_sensors/pull/4</a></p><p><img class="img-zoomable" src="6458291605556935143.jpg" alt=""/><img class="img-zoomable" src="3454001605556945627.png" alt=""/></p></description></item><item><title>Emotional Core: temp impacts + refactoring</title><link>https://blog.agramakov.me/posts/2020/10-28-emotional-core-temp-impacts-refactoring/</link><pubDate>Wed, 28 Oct 2020 15:04:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/10-28-emotional-core-temp-impacts-refactoring/</guid><description><p>Just finished a huge update for the Core. New feature is temporary impacts. It simulates a situation, that you can get watching<a href="https://www.youtube.com/watch?v=BpmTtWC5BJU" target="_blank">a screamer</a> (Caution! ). Something scarry is happening, you are getting a lot of adrenaline and cortisol, after 5 minutes you are ok.</p><p>Also, the structure of the core was dramatically simplified. On the picture temporary impacts are called Conscious Data because I&rsquo;m going to use it to simulate &ldquo;bad thoughts&rdquo; of the robot.</p><p><img class="img-zoomable" src="4452771603897229885.png" alt=""/>
Update is here (there is an example you can try :) ):</p><p><a href="https://github.com/an-dr/r_giskard/tree/4e7e5115e7975b12e065ffd98130c0e9ba97f551/emotional_core" target="_blank">https://github.com/an-dr/r_giskard/tree/4e7e5115e7975b12e065ffd98130c0e9ba97f551/emotional_core</a></p></description></item><item><title>Emotional Core: in details</title><link>https://blog.agramakov.me/posts/2020/10-16-emotional-core-in-details/</link><pubDate>Fri, 16 Oct 2020 21:06:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/10-16-emotional-core-in-details/</guid><description><p>Now after some polishing of the basic emotional core I can tell about how it works and about it&rsquo;s structure.</p><p>Emotional core consists of three main parts (see the picture bellow):</p><ul><li>Input Data Descriptors - describes data from sensors and how it should affect the core</li><li>Emotional States Descriptors - named states of the core described by specific values of core parameters</li><li>Core State - contains core parameters, value of sensors and pointer to relevant to parameters Emotional States Descriptors</li></ul><p><img class="img-zoomable" src="5680781602881967541.png" alt=""/>
Or more detailed:</p><p><img class="img-zoomable" src="6296431602882202035.png" alt=""/>
When you write the data, the core does the following:</p><ul><li>updates saved sensor value</li><li>updates core parameter: param += (new_sens_val - old_sens_val) * weight</li><li>updates the current core state based on updated parameters</li></ul><p>Here are some examples of data that could be used with the core.</p><p>Example of core parameters:</p><p>[ &ldquo;cortisol&rdquo;,</p><p>&ldquo;dopamine&rdquo;,</p><p>&ldquo;adrenaline&rdquo;,</p><p>&ldquo;serotonin&rdquo; ]
Example of a core&rsquo;s emotional state:</p><p>{</p><p>&ldquo;name&rdquo;: happiness,</p><p>&ldquo;conditions&rdquo;: [</p><p>{</p><p>&ldquo;param&rdquo;: &ldquo;cortisol&rdquo;,</p><p>&ldquo;op&rdquo;: LESS_THAN,</p><p>&ldquo;value&rdquo;: 10</p><p>},</p><p>{</p><p>&ldquo;param&rdquo;: &ldquo;serotonin&rdquo;,</p><p>&ldquo;op&rdquo;: GREATER_THAN,</p><p>&ldquo;value&rdquo;: 100</p><p>}</p><p>]</p><p>}
Example of input data descriptor:</p><p>{</p><p>&ldquo;sensor_name&rdquo;: &ldquo;temperature sensor&rdquo;,</p><p>&ldquo;val_min&rdquo;: 0,</p><p>&ldquo;val_max&rdquo;: 255,</p><p>&ldquo;weights&rdquo;: [</p><p>{</p><p>&ldquo;core_param_name&rdquo;: &ldquo;serotonin&rdquo;,</p><p>&ldquo;weight&rdquo;: 0.5</p><p>},</p><p>{</p><p>&ldquo;core_param_name&rdquo;: &ldquo;cortisol&rdquo;,</p><p>&ldquo;weight&rdquo;: -0.5</p><p>}</p><p>]</p><p>}
Input data example:</p><p>{</p><p>&ldquo;sensor_name&rdquo;: &ldquo;temperature sensor&rdquo;,</p><p>&ldquo;value&rdquo;: 120</p><p>}
My next step will be in looking appropriate parameters and weights that can adequately describe (roughly of course) some human emotions.</p><p>Source core:<a href="https://github.com/an-dr/r_giskard" target="_blank">https://github.com/an-dr/r_giskard</a></p></description></item><item><title>The Emotional Core Test (attention: pretty boring!)</title><link>https://blog.agramakov.me/posts/2020/09-26-the-emotional-core-test-attention-pretty-boring/</link><pubDate>Sat, 26 Sep 2020 23:02:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/09-26-the-emotional-core-test-attention-pretty-boring/</guid><description><p>Testing the new Emotional Core for the R.Giskard project</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/YS33SozeLH4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>The code is here:<a href="https://github.com/an-dr/r_giskard/tree/feature/emotional_core" target="_blank">https://github.com/an-dr/r_giskard/tree/feature/emotional_core</a></p></description></item><item><title>Emotional Core Sketch</title><link>https://blog.agramakov.me/posts/2020/09-19-emotional-core-sketch/</link><pubDate>Sat, 19 Sep 2020 18:24:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/09-19-emotional-core-sketch/</guid><description><p>Well,<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">the third milestone is in progress</a>.</p><p>It&rsquo;s time for the most interesting part of the project - emotions and reflexes. Now, let&rsquo;s talk about emotions, but in short.</p><p>In my view and my understanding of what should be going on here, emotions - it is just the name of our mental state. We are going to have a set of such emotional states - to use those states as a simple indicator for our systems using my emotions implementation.</p><p>In the animal case, each state is defined by the specific chemical condition of the brain. The virtual analog of those chemical components probably should be a set of parameters.</p><p>Input sensorial data could affect the parameters with different weights. E.g. the huger amount of light should decrease the parameter reflecting the level of anxiety.
If the level of anxiety is low enough, we could declare that the code in a calm state.</p><p>The pretty rough structure of the emotional core is on the picture:</p><p><img class="img-zoomable" src="4753741600626101826.png" alt=""/>
The work is going at the branch of my r_giskard repository, where I&rsquo;m going to implement and the core and the simulator to test it on desktop systems:</p><p><a href="https://github.com/an-dr/r_giskard/tree/feature/simulator" target="_blank">https://github.com/an-dr/r_giskard/tree/feature/simulator</a></p><p>I&rsquo;m implementing it in C++ because in that scenario the core would be used in embedded systems as well as in python programs.</p></description></item><item><title>Zakhar Milestone: Zakharos</title><link>https://blog.agramakov.me/posts/2020/08-29-zakhar-milestone-zakharos/</link><pubDate>Sat, 29 Aug 2020 21:01:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-29-zakhar-milestone-zakharos/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/CtFNXtHARPk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Changelog since the<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/178126-the-reptile-demo" target="_blank">Reptile Demo</a> Milestone:</p><p><strong>Common:</strong></p><ul><li>Updated cable system with 8-pin connectors (see:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/181955-power-post-part-one" target="_blank">Power post. Part one.</a>).</li><li>Added union connectors for charging of all platforms at once.</li><li>Added switchers for each platform.</li><li>Created a new repository including the sensor platform and the face module:<a href="https://github.com/an-dr/zakhar_io" target="_blank">zakhar_io</a></li></ul><p><strong>Sensor platform:</strong></p><ul><li>Used Arduino Nano v3 instead Pro Micro - for better stability</li><li>United with face module as one platform</li><li>Added Ultrasonic Sensor HC-SR04, but without software support yet.</li><li>Updated face module with stabilized voltage</li></ul><p><strong>Computing (brain) platform:</strong></p><ul><li>Raspberry Pi 4, ROS Noetic with Python 3</li><li>New<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179474-improved-ros-based-architecture-for-the-program-core" target="_blank">ROS-based application architecture</a></li><li>OLED as status display for the computing platform (see:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/182936-startup-check" target="_blank">Startup check</a>)</li><li>Achieved stable (finally) I2C connection. Thanks to lower frequency (10KHz) and the new connectors.</li><li>Added Raspberry Pi Camera, but without software support yet.</li><li>Created a new repository for the platform:<a href="https://github.com/an-dr/zakhar_brain" target="_blank">an-dr/zakhar_brain: Software for Zakhar&rsquo;s brain</a></li></ul><p><strong>Moving platform:</strong></p><ul><li>New chassis with a stand</li><li>ESP32 instead of Arduino Nano v3</li><li>Bluetooth connection control</li><li>Position module MPU9250 with ability to turn for a given angle (see:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/180662-new-esp32-based-platform-testing-angles" target="_blank">New ESP32-based platform testing: Angles!</a>)</li><li>New third wheel - with rubber</li><li>Removed rotation encoders as useless</li><li>Updated motors by the modification with a metal reductor</li></ul><p>Repository:<a href="https://github.com/an-dr/zakhar" target="_blank">https://github.com/an-dr/zakhar</a></p></description></item><item><title>Startup check</title><link>https://blog.agramakov.me/posts/2020/08-29-startup-check/</link><pubDate>Sat, 29 Aug 2020 19:42:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-29-startup-check/</guid><description><p>The startup system check is shown on an OLED display. The display has its separated i2c bus (I2C-3) to communicate with the Raspberry.</p><p>The startup process is the following:</p><ul><li>Greeting</li><li>Test I2C-1 devices: moving platform, sensor platform, and face module. If something is not represented on the bus, the error will be shown.</li><li>Waiting while ssh service will be loaded</li><li>Waiting while the network will be connected</li><li>Waiting while roscore will be launched</li><li>Infinite loop showing the network and project info. The robot is ready!</li></ul><p>Gif of the process:</p><p><img class="img-zoomable" src="https://media4.giphy.com/media/Ie4Qa31pFDiuYpS7Fg/giphy.gif" alt=""/>
Code of the startup check process:</p><p><a href="https://github.com/an-dr/zakhar_service/tree/feature/display_n_startup/display" target="_blank">https://github.com/an-dr/zakhar_service/tree/feature/display_n_startup/display</a></p></description></item><item><title>Noctural</title><link>https://blog.agramakov.me/posts/2020/08-15-noctural/</link><pubDate>Sat, 15 Aug 2020 23:34:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-15-noctural/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/gB2MD0bJi-k" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>[Bad Idea, canceled] Power post. Part one.</title><link>https://blog.agramakov.me/posts/2020/08-08-bad-idea-canceled-power-post-part-one/</link><pubDate>Sat, 08 Aug 2020 15:14:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-08-bad-idea-canceled-power-post-part-one/</guid><description><p><img class="img-zoomable" src="5120251596899760195.png" alt=""/></p><p>There are a lot of issues when you connect low- and high-current devices to the same electrical circuit. Basically, you are getting:</p><ul><li><p>Noise at sensors&rsquo; output</p></li><li><p>Freezes and resets of MCUs</p></li><li><p>Artefacts on LCD and backlight&rsquo;s flashing</p></li><li><p>Other unexpected issues</p></li></ul><p>That&rsquo;s really annoying that&rsquo;s why I spend last several weeks thinking how to implement power supply for Zakhar. There are two things I want from this update: to avoid things mentioned above and to simplify development allowing me to work with every platform separately. There is also thing that I don&rsquo;t want, to develop a reliable power system. Please get me right, it is cool and interesting, but it is not something new. So, what I want is:</p><ul><li><p>Use as many ready-to-use components</p></li><li><p>Make an isolated power supply system for each platform</p></li></ul><p>Sounds like power banks!</p><p>I&rsquo;ve been already using a cool power bank with two low- and high-current ports for motors and the rest electronics:</p><p><img class="img-zoomable" src="8165941596898390849.jpg" alt=""/>
I will use it for the moving platform only.</p><p>One months ago, I&rsquo;ve already added another one for the computing platform with a Raspberry:</p><p><img class="img-zoomable" src="4828081596898493713.jpg" alt=""/>
Since the face module and the sensor platform haven&rsquo;t been separated almost never during the development, I decided to add to them only single common power bank. Unfortunately, I haven&rsquo;t found anything, so I had to make own one:</p><p><img class="img-zoomable" src="9793171596898834447.jpg" alt=""/>
(I know, LEGO AGAIN!)</p><p>Okey. Now I must charge 3 power banks. Easy! Since we are not interested of using four pins of inter-platform connectors for power delivery anymore it&rsquo;s time to update the connectors for delivery of charge to power banks.</p><p>Again, to not spending the time for development of physical connectors, let&rsquo;s use best practices.</p><ol><li>Take breadboards:</li></ol><p><img class="img-zoomable" src="3939441596899318766.jpg" alt=""/>
2. Think a little</p><p><img class="img-zoomable" src="5444821596899369366.png" alt=""/>
3. Cut!</p><p><img class="img-zoomable" src="2719511596899494609.jpg" alt=""/>
And here what and how we can connect:</p><p><img class="img-zoomable" src="184951596899577714.png" alt=""/>
As a result, two platforms connected look like:</p><p><img class="img-zoomable" src="5555441596899880139.jpg" alt=""/>
And no more blinking of LCD backlight while the Raspberry is thinking, or motors are turning!</p><p>I&rsquo;m waiting a bunch of cables and switchers so next time I&rsquo;ll show the final variant of Zakhar&rsquo;s power supply solution and we, having a stable and plug-and-play set of flatform will continue to investigate the life behavior and how to implement it for the robot!</p><p>Stay tuned!</p><p>P.S If you have any better solutions let me know!</p></description></item><item><title>Turns: -45, +45, -90, +90 degrees</title><link>https://blog.agramakov.me/posts/2020/07-21-turns-45-45-90-90-degrees/</link><pubDate>Tue, 21 Jul 2020 17:07:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-21-turns-45-45-90-90-degrees/</guid><description><p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/Raywkmo5Lf8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div>
Updated code here:<a href="https://github.com/an-dr/zakhar_platform/pull/2" target="_blank">https://github.com/an-dr/zakhar_platform/pull/2</a></p></description></item><item><title>Assembled with the new moving platform! Turn 90 degree</title><link>https://blog.agramakov.me/posts/2020/07-19-assembled-with-the-new-moving-platform-turn-90-degree/</link><pubDate>Sun, 19 Jul 2020 18:43:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-19-assembled-with-the-new-moving-platform-turn-90-degree/</guid><description><p>From the Raspberry the moving platform gets an argument 0x5a (90) degrees, then a command to turn right. If the argument is 0x00 command is executing during 100ms then the platform stops.</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/_nnMFdBr9SM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>New ESP32-based platform testing: Angles!</title><link>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-angles/</link><pubDate>Sat, 11 Jul 2020 18:20:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-angles/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/Z1AaA5RbPfY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Works:</p><ul><li>GY-91 through I2C: gyroscope + accelerometer</li><li>Serial for external control</li><li>I2C slave for external control</li><li>Wireless connection - Bluetooth serial port</li><li>Motors control</li><li>PWM for motors - three speeds</li><li>RPM encoders</li><li>Position data handling - calibration, filtering, conversion in angles</li></ul><p>To-do:</p><ul><li>Code refactoring and configuration with<a href="https://en.wikipedia.org/wiki/Menuconfig" target="_blank">menuconfig</a></li></ul><p>Work in progress here:<a href="https://github.com/an-dr/zakhar_platform/pull/1" target="_blank">Feature/esp32 by an-dr Pull Request #1 an-dr/zakhar_platform</a></p></description></item><item><title>New ESP32-based platform testing: Updates!</title><link>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-updates/</link><pubDate>Sat, 11 Jul 2020 13:12:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-updates/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/SugTDM9RSro" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Works:</p><ul><li>GY-91 through I2C: gyroscope + accelerometer</li><li>Serial for external control</li><li>I2C slave for external control</li><li>Wireless connection - Bluetooth serial port</li><li>Motors control</li><li>PWM for motors - three speeds</li><li>RPM counts. (BTW the motors have different speed as it turned out )</li></ul><p>To-do:</p><ul><li>Position data handling - calibration, filtering, conversion in angles (?)</li><li>Code refactoring and configuration with<a href="https://en.wikipedia.org/wiki/Menuconfig" target="_blank">menuconfig</a></li></ul><p>Work in progress here:<a href="https://github.com/an-dr/zakhar_platform/pull/1" target="_blank">Feature/esp32 by an-dr Pull Request #1 an-dr/zakhar_platform</a></p></description></item><item><title>New ESP32-based platform testing</title><link>https://blog.agramakov.me/posts/2020/07-10-new-esp32-based-platform-testing/</link><pubDate>Fri, 10 Jul 2020 18:03:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-10-new-esp32-based-platform-testing/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/6LgkI2rKElI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Works:</p><ul><li>GY-91 through I2C: gyroscope + accelerometer</li><li>Serial for external control</li><li>I2C slave for external control</li><li>Motors control</li></ul><p>To-do:</p><ul><li>Wireless connection</li><li>Position data handling - calibration, filtering, conversion in angles (?)</li><li>PWM for motors GPIO</li><li>RPM counts</li></ul><p>Work in progress here:<a href="https://github.com/an-dr/zakhar_platform/pull/1" target="_blank">Feature/esp32 by an-dr Pull Request #1 an-dr/zakhar_platform</a></p></description></item><item><title>Zakhar disassembled</title><link>https://blog.agramakov.me/posts/2020/07-06-zakhar-disassembled/</link><pubDate>Mon, 06 Jul 2020 21:04:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-06-zakhar-disassembled/</guid><description><p>While the robot is changing all the time, after the last update I&rsquo;ve decided to take some snapshot of its current state and show to the public what&rsquo;s going on now. Let&rsquo;s start:</p><p><img class="img-zoomable" src="9968151594067608724.png" alt=""/>
Zakhar consists of three (sometimes four) platforms and a service OLED display. I&rsquo;m using the last one for debugging, testing, etc.</p><p><strong>Moving platform</strong></p><p>The moving platform just got the update with ESP32-board as a replacement for old Arduino and with the MPU module</p><p><img class="img-zoomable" src="8897821594067828971.png" alt=""/>
I did it for the sake of better multitasking which is needed to me for simpler handling of data from sensors and commands from the computing platform.</p><p>The platform is carrying on a power bank which supplies motors of the platform and all other electronics of Zakhar (except Raspberry Pi).</p><p>The power, as well as commands, are transmitting through unified 8-pin connectors (2 pins for I2C, 2 pins for 5v of low current, 2 pins for 5v of high current, 2 pins - reserved)</p><p><strong>Computing platform</strong></p><p>The computing platform has Raspberry Pi 4B with 4Gb of RAM and it&rsquo;s own power bank - for a stable supply of the Raspberry.</p><p><img class="img-zoomable" src="7773861594068663569.png" alt=""/><strong>Face+sensor platform</strong></p><p>The platform is carrying an Arduino which performs as an I2C device with data collected from sensors (i have only one now) into I2C registers and the face module.</p><p><img class="img-zoomable" src="9146271594068881703.png" alt=""/>
Inside the face module, there are another Arduino and a DC-DC convertor (for LCD supplying)</p><p><img class="img-zoomable" src="7523901594069285373.jpg" alt=""/><img class="img-zoomable" src="1412781594069239376.png" alt=""/>
That&rsquo;s it, thank you for your attention!</p><p>I&rsquo;m going back to writing the firmware for ESP32. CU soon!</p></description></item><item><title>Prototype of a new moving platform. Work in progress</title><link>https://blog.agramakov.me/posts/2020/07-03-prototype-of-a-new-moving-platform-work-in-progress/</link><pubDate>Fri, 03 Jul 2020 22:28:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-03-prototype-of-a-new-moving-platform-work-in-progress/</guid><description><p>Planned: ESP32, gyroscope, accelerometer, onboard data processing, variable speed, wireless mode and more!</p><p><img class="img-zoomable" src="255951593815161439.jpeg" alt=""/><a href="https://github.com/an-dr/zakhar_platform/tree/feature/esp32" target="_blank">https://github.com/an-dr/zakhar_platform/tree/feature/esp32</a></p></description></item><item><title>Some zakharos_core documentation</title><link>https://blog.agramakov.me/posts/2020/06-30-some-zakharos_core-documentation/</link><pubDate>Tue, 30 Jun 2020 18:30:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-30-some-zakharos_core-documentation/</guid><description><p>Before moving to the reimplementing of moving platform on ESP32 I&rsquo;ve added some documentation to the<a href="https://github.com/an-dr/zakharos_core" target="_blank">zakharos_core repository</a>. Code has a lot of roughness that will be polished later (probably on the next milestone -<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">The Emotions Demo</a>)</p><p>Documentation is not complete but will be supplemented with time. Probably the illustration, requirements and how to build should be enough to check the implementation out. So PTAL if you are interested!</p><p>Repository:<a href="https://github.com/an-dr/zakharos_core" target="_blank">an-dr/zakharos_core</a></p><p>The code inaction:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179932-reimplemented-the-reptile-demo-with-ros" target="_blank">Reimplemented the Reptile demo with ROS!</a></p><p><img class="img-zoomable" src="3999271593540979159.f1c5e4f228a0765ffc63961102342a7a" alt=""/></p></description></item><item><title>Reimplemented the Reptile demo with ROS!</title><link>https://blog.agramakov.me/posts/2020/06-29-reimplemented-the-reptile-demo-with-ros/</link><pubDate>Mon, 29 Jun 2020 20:37:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-29-reimplemented-the-reptile-demo-with-ros/</guid><description><p>Half-milestone. What&rsquo;s new for now:</p><ul><li>Brand new moving platform</li><li>United sensor/face platform</li><li>Raspberry Pi 4</li><li>New<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179474-improved-ros-based-architecture-for-the-program-core" target="_blank">ROS-based application architecture</a></li><li>Updated face module with stabilized voltage</li><li>Updated cable system with 8-pin connectors</li><li>Stable (finally) I2C connection. Thanks to lower frequency (10KHz) and the new connectors.</li></ul><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/FIUM3jjPLFA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>Out of memory: moving to RPi4</title><link>https://blog.agramakov.me/posts/2020/06-25-out-of-memory-moving-to-rpi4/</link><pubDate>Thu, 25 Jun 2020 17:59:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-25-out-of-memory-moving-to-rpi4/</guid><description><p>Ok,<a href="https://www.ros.org/" target="_blank">ROS</a> needs RAM.</p><p>Just received Raspberry Pi 4 B with 4 GB of RAM and finally I can continue implementation of<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179474-improved-ros-based-architecture-for-the-program-core" target="_blank">this structure</a>. Stay tuned!</p><p><img class="img-zoomable" src="2743011593107962568.jpg" alt=""/></p></description></item><item><title>Improved ROS-based architecture for the program core</title><link>https://blog.agramakov.me/posts/2020/06-18-improved-ros-based-architecture-for-the-program-core/</link><pubDate>Thu, 18 Jun 2020 21:34:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-18-improved-ros-based-architecture-for-the-program-core/</guid><description><p>I&rsquo;m about to finish migration to the ROS. Aside small things, architecture seems crystalized. It looks like following:</p><p><img class="img-zoomable" src="344831592515514675.png" alt=""/>
Every block represents ROS-node (except the hardware block). Interaction between blocks is trying to mimic a real brain (as I understand what is happening there).</p><p>I&rsquo;m lazy to describe every node right now, I will write an article when this work will end up with the<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">second demo</a>. But if you interested you can check the code out in the repository here:</p><p><a href="https://github.com/an-dr/zakharos_core" target="_blank">https://github.com/an-dr/zakharos_core</a></p><p>More about ROS:<a href="http://wiki.ros.org/" target="_blank">http://wiki.ros.org/</a></p></description></item><item><title>Just a photo with current state and a repo I'm working on.</title><link>https://blog.agramakov.me/posts/2020/06-11-just-a-photo-with-current-state-and-a-repo-im-working-on/</link><pubDate>Thu, 11 Jun 2020 19:03:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-11-just-a-photo-with-current-state-and-a-repo-im-working-on/</guid><description><p><img class="img-zoomable" src="2648581591903542559.jpg" alt=""/><a href="https://github.com/an-dr/zakharos_core" target="_blank">https://github.com/an-dr/zakharos_core</a> (just pushed one more step according<a href="https://github.com/an-dr/zakhar/blob/master/README.md#milestones" target="_blank">the plan</a>)</p></description></item><item><title>Moving platform update 1</title><link>https://blog.agramakov.me/posts/2020/06-06-moving-platform-update-1/</link><pubDate>Sat, 06 Jun 2020 08:44:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-06-moving-platform-update-1/</guid><description><p>When three wheels are better than four</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/inOzUCIyt_U" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>Thoughts about execution of commands in a mind-like program</title><link>https://blog.agramakov.me/posts/2020/05-31-thoughts-about-execution-of-commands-in-a-mind-like-program/</link><pubDate>Sun, 31 May 2020 20:36:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-31-thoughts-about-execution-of-commands-in-a-mind-like-program/</guid><description><p>I&rsquo;m trying to reverse engineer myself and apply it to the Zakhar. Some points:</p><ol><li>I can&rsquo;t send a direct order to, let say, my hand</li><li>I must form a concept, like &ldquo;fold fingers into a fist&rdquo; or &ldquo;move the hand up&rdquo;</li></ol><p>What that&rsquo;s mean for Zakhar:</p><ul><li>The main program should use high-level commands or concept</li><li>It is needed one more node translating these concepts into commands for devices</li></ul><p>Possible in-program interaction using terms of ROS and Zakhar should be like:</p><p><img class="img-zoomable" src="3901601590956653230.jpg" alt=""/>
The thoughts are related to:<a href="https://github.com/an-dr/zakharos_core" target="_blank">https://github.com/an-dr/zakharos_core</a></p></description></item><item><title>The ​Zakharos Milestone</title><link>https://blog.agramakov.me/posts/2020/05-31-the-zakharos-milestone/</link><pubDate>Sun, 31 May 2020 09:46:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-31-the-zakharos-milestone/</guid><description><p>Started the work on the next milestone called Zakharos. Aims of the milestone:</p><ul><li>Re-implementing The Reptile Demo with<a href="https://www.ros.org/" target="_blank">ROS</a> - for simpler development of sophisticated Mind&rsquo;s structures</li><li>Re-implementing Moving platform with the ESP32 , an accelerometer, PWM, FreeRTOS and WIFI communication - for faster responses, precise positioning, simpler access and more stable working.</li></ul><p><a href="https://github.com/an-dr/zakhar#milestones" target="_blank">Project&rsquo;s milestones</a></p></description></item><item><title>Robot with the Conscious: Imitating animal behavior for reducing user's anxiety</title><link>https://blog.agramakov.me/posts/2020/05-29-robot-with-the-conscious-imitating-animal-behavior-for-reducing-users-anxiety/</link><pubDate>Fri, 29 May 2020 18:59:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-29-robot-with-the-conscious-imitating-animal-behavior-for-reducing-users-anxiety/</guid><description><p>Finally! Just finished an article about my robot.</p><p><a href="https://blog.agramakov.me/2020/05/29/robot-with-the-conscious/" target="_blank">Robot with the Conscious: Imitating animal behavior for reducing user s anxiety</a></p><p>#robot #article #robotics #embedded #zakhar #hardware #automation #engineering #hobby #diyproject #embeddedsystems #electronics</p><p><img class="img-zoomable" src="9575731590778747528.png" alt=""/></p></description></item><item><title>Moving to the Robot Operating System</title><link>https://blog.agramakov.me/posts/2020/05-27-moving-to-the-robot-operating-system/</link><pubDate>Wed, 27 May 2020 22:57:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-27-moving-to-the-robot-operating-system/</guid><description><p>The first experiments with ROS showed that it probably will help a lot in building mind-like organized programs.</p><p>Node-based conception and server-client interactions hopefully allow me to think less about queues, threads, and access control, which took the most of the time during building the first demo on bare Python.</p><p>Here are some ideas of the mind-like program architecture. I probably will working on it next weekend.</p><p><img class="img-zoomable" src="4773851590620014679.png" alt=""/>
Colored blocks are nodes</p></description></item><item><title>The Reptile Demo</title><link>https://blog.agramakov.me/posts/2020/05-25-the-reptile-demo/</link><pubDate>Mon, 25 May 2020 15:17:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-25-the-reptile-demo/</guid><description><p>While I&rsquo;m finishing a new article about the project, please look the video demonstration. Without the context for now</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/xyb9NgWsHNY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item></channel></rss>