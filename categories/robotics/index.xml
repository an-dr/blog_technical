<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>robotics on technical_</title><link>https://blog.agramakov.me/categories/robotics/</link><description>Recent content in robotics on technical_</description><generator>Hugo</generator><language/><lastBuildDate>Sun, 10 Sep 2023 19:15:36 +0000</lastBuildDate><atom:link href="https://blog.agramakov.me/categories/robotics/index.xml" rel="self" type="application/rss+xml"/><item><title>LeOn, a Cat Companion. Architecture First.</title><link>https://blog.agramakov.me/posts/2023/09-09-leon-robotic-cat-companion-architecture-first/</link><pubDate>Sun, 10 Sep 2023 19:15:36 +0000</pubDate><guid>https://blog.agramakov.me/posts/2023/09-09-leon-robotic-cat-companion-architecture-first/</guid><description><p>With this post, I start my new robotic project. This project will have several important features:</p><ul><li>Architecture-oriented<ul><li>Develop the right architecture using architectural best practices</li></ul></li><li>Result-oriented and minimalistic<ul><li>Progress over perfection</li></ul></li><li>Integration-oriented.<ul><li>Reusing existing libraries, projects, etc. Minimum development of individual pieces</li></ul></li><li>Reusable in my other projects<ul><li>It should be done automatically with a good architecture. But I want to make several special decisions to make the project easy to transfer to my another robotic project</li></ul></li></ul><h1 id="what-is-the-project-about">What is the project about?</h1><p>It is a cat companion. A simple robot that can move around, avoid big obstacles and move a stick for playing with a cat.</p><h1 id="what-is-the-first-step">What is the first step?</h1><p>I‚Äôm starting with the architecture development. I will try to make the most abstract decisions first, leaving the implementation details to the very end.</p><p>The first step is development of the use cases. Basically, there are 3 actors for this robot - a cat, a cat owner, and me, a robotic developer. As a developer, It is important for me to have a moving platform (legs, wheels, etc.) reusable by other projects, so it is also reflected in my use case diagram:</p><p><img class="img-zoomable" src="usecase.drawio.svg" alt="usecase"/></p><p>That is it for now. Next time we will probably define the system requirements based on each use case.</p></description></item><item><title>Zakhar Brain Service</title><link>https://blog.agramakov.me/posts/2022/09-18-zakhar-brain-service/</link><pubDate>Sun, 18 Sep 2022 21:48:21 +0100</pubDate><guid>https://blog.agramakov.me/posts/2022/09-18-zakhar-brain-service/</guid><description><p>Yesterday I merged a big software update to the Zakhar&rsquo;s Raspberry Pi Unit -<code>brain_service</code>.</p><p>The update brings a service providing the robot&rsquo;s status (network, OS status) and access to the CAN bus for many simultaneously connected clients. Also, the service tracks the presence of other robot Units on the CAN bus.</p><p>There is another service shipping with the same repository<code>brain_service_display</code>. This service connects to<code>brain_service</code> and shows some important information on a small OLED display:</p><p><img class="img-zoomable" src="screen1.jpg" alt=""/><img class="img-zoomable" src="screen2.jpg" alt=""/><img class="img-zoomable" src="screen3.jpg" alt=""/></p><ul><li>f - Face Unit</li><li>S - Sensor Unit</li><li>m - Wheeled moving platform (&ldquo;Motors&rdquo;)</li><li>t - Service Unit (&ldquo;Tool&rdquo;)</li></ul><p>The service structure is presented below:</p><p><img class="img-zoomable" src="structure.svg" alt=""/></p><p>The service publishes status information and incoming CAN messages to<a href="https://zeromq.org/" target="_blank">ZeroMQ</a> subscribers. The service allows all TCP connections including connections from external machines. See<a href="https://github.com/Zakhar-the-Robot/brain_pycore" target="_blank">brain_pycore</a> for compatible subscriber and client implementation classes.</p><p>Sources:</p><ul><li><a href="https://github.com/Zakhar-the-Robot/brain_service" target="_blank">https://github.com/Zakhar-the-Robot/brain_service</a></li><li><a href="https://github.com/Zakhar-the-Robot/brain_pycore" target="_blank">https://github.com/Zakhar-the-Robot/brain_pycore</a></li></ul></description></item><item><title>Brain Software Architecture</title><link>https://blog.agramakov.me/posts/2022/08-31-brain-software-architecture/</link><pubDate>Wed, 31 Aug 2022 19:58:42 +0100</pubDate><guid>https://blog.agramakov.me/posts/2022/08-31-brain-software-architecture/</guid><description><p>Before updating Alive OS to support<a href="https://zakhar-the-robot.github.io/doc/docs/communication-protocols/canbus/" target="_blank">qCAN (my CANbus-based protocol)</a> I have the last thing to do. To simplify my live in future I need a CAN publisher that can publish messages to many subscribers. My main subscribe of course is AliveOS but also to display information about connected devices I need a second subscriber - a service listening only qCAN Present messages.</p><p>To do it I will use a<a href="https://zeromq.org/" target="_blank">ZeroMQ protocol</a> - an extremely supported and documented for many programming languages standard. I&rsquo;m going to update my brain_service to support the protocol and it will be responsible for all interaction with Raspberry Pi.</p><p><img class="img-zoomable" src="Brain_Structure.png" alt=""/></p><p>Repositories:</p><ul><li><a href="https://github.com/Zakhar-the-Robot/brain_pycore" target="_blank">https://github.com/Zakhar-the-Robot/brain_pycore</a></li><li><a href="https://github.com/Zakhar-the-Robot/brain_service" target="_blank">https://github.com/Zakhar-the-Robot/brain_service</a></li></ul></description></item><item><title>Sensor Unit with qCAN Support is Merged!</title><link>https://blog.agramakov.me/posts/2022/08-17-sensor-unit-with-qcan-support-is-merged/</link><pubDate>Wed, 17 Aug 2022 19:53:46 +0100</pubDate><guid>https://blog.agramakov.me/posts/2022/08-17-sensor-unit-with-qcan-support-is-merged/</guid><description><p><img class="img-zoomable" src="2.jpg" alt=""/></p><p>Updated code for the Sensor Unit with qCAN support is merged and documented!</p><ul><li>Source code:<a href="https://github.com/Zakhar-the-Robot/io_sensors" target="_blank">https://github.com/Zakhar-the-Robot/io_sensors</a></li><li>Documentation:<a href="https://zakhar-the-robot.github.io/doc/docs/systems/io_sensors/" target="_blank">https://zakhar-the-robot.github.io/doc/docs/systems/io_sensors/</a></li></ul><p><img class="img-zoomable" src="1.jpg" alt=""/></p></description></item><item><title>The Wheeled Platform with CAN support is merged!</title><link>https://blog.agramakov.me/posts/2022/08-09-the-wheeled-platform-with-can-support-is-merged/</link><pubDate>Tue, 09 Aug 2022 19:45:42 +0100</pubDate><guid>https://blog.agramakov.me/posts/2022/08-09-the-wheeled-platform-with-can-support-is-merged/</guid><description><p>A new step in Zakhar global transition to CAN bus is done!</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/0DVqdEvbaGk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>The unit consists of:</p><ul><li>ESP32 DevKit V1 - 36 pins</li><li>GY-91 - MPU module</li><li>HW-021 TJA1050 - CAN-bus driver</li><li>Red-Green LED module</li></ul><p>It can be controlled via CANbus, Serial and Bluetooth.</p><p>The documentation page is available here:</p><p><a href="https://zakhar-the-robot.github.io/doc/docs/systems/motion/wheeled-platform/" target="_blank">https://zakhar-the-robot.github.io/doc/docs/systems/motion/wheeled-platform/</a></p></description></item><item><title>New Zakhar Face Module</title><link>https://blog.agramakov.me/posts/2022/08-07-new-zakhar-face-module/</link><pubDate>Sun, 07 Aug 2022 21:44:38 +0100</pubDate><guid>https://blog.agramakov.me/posts/2022/08-07-new-zakhar-face-module/</guid><description><p><img class="img-zoomable" src="IMG_20220807_210614803.jpg" alt=""/></p><p>The new Unit is based on the ESP-Wrover-Kit and uses a TJA1050 module for CANbus communication.</p><p>It is compatible with the qCAN protocol (described on the Zakhar main page) and can show 5 facial expressions: Anger (0x32), Sadness (0x34), Pleasure (0x33), a Blink (0x31), and Calm (0x30).</p><p>To control it you should send a CAN message with an expression code using a 125Kbit standard CAN frame and with the message id ending with byte 0x4. E.g. send a message with ID: 0x004, data: 0x33 to make Zakhar happy üòç.</p><p><a href="https://zakhar-the-robot.github.io/doc/docs/systems/io/face/" target="_blank">Read more on the Zakhar main page</a>.</p></description></item><item><title>CAN Bus and a New Simple Protocol</title><link>https://blog.agramakov.me/posts/2022/07-21-can-bus-and-a-new-simple-protocol/</link><pubDate>Thu, 21 Jul 2022 20:28:27 +0100</pubDate><guid>https://blog.agramakov.me/posts/2022/07-21-can-bus-and-a-new-simple-protocol/</guid><description><h2 id="not-only-two-wires">Not Only Two Wires</h2><p>It‚Äôs been more than two years already since I started working on my robot Zakhar. The Zakhar 1 was built out of Lego and relied on I2C communication between modules. It was a nightmare because as it turned out each MCU developer has its own understanding of how a developer should interact with the I2C unit. What I wanted from the interface:</p><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/Untitled.png" alt="I2C makes robots sad"/></p><ul><li>Main/secondary nodes interaction</li><li>Understand what devices are in the network</li><li>Send commands</li><li>Receive sensor data</li></ul><p>To make all this possible I‚Äôve developed a simple I2C-based protocol but couldn‚Äôt get the desired behavior (read more in<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/196935-dear-i2c-i-resemble" target="_blank">my post</a>) on all platforms - Raspberry Pi, STM32, Arduino, and ESP32. Overly Zakhar 1 had too many problems to continue the project - mechanically it was floppy, power sources were unstable, the display was flickering, and the interface‚Ä¶ It was too distracting, unstable, and annoying to continue!</p><p>I started Zakhar 2 based on the CAN interface. Why CAN? There are many advantages of CAN compared to I2C:</p><ul><li>Better MCU developer support</li><li>A single data frame contains much more information than I2C</li><li>Physical layer - actually not specified, but de-facto it is RS485 and it is much suitable for heterogeneous systems with several power sources:<ul><li>Really two-wires interface, without a common ground</li><li>No voltage levels shifters</li></ul></li><li>Embedded addressing mechanism</li><li>Still simple</li></ul><p>There are cons of course. The main one is that not all platforms have hardware CAN bus support. But there are many external modules on the market, there are tons of drivers developed for all platforms, and there are thousands of guides written, so it is easy to mitigate.</p><h2 id="yes-a-new-protocol">Yes, a new protocol!</h2><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/Untitled%201.png" alt="CAN is used in vehicles and has many flavors"/></p><p>I like developing standards. I really wanted to invent my own bicycle. Also, I wanted to make a transition from I2C to CAN smooth keeping all implemented features and continuing established directions. And likely for Zakhar 2, there was no good candidate. Had no reason not to develop my own CAN-based protocol. Here is not the full list of my options with the main reason to say no:</p><ul><li><a href="https://en.wikipedia.org/wiki/CANopen" target="_blank">CANopen</a> - too complex</li><li><a href="https://en.wikipedia.org/wiki/DeviceNet" target="_blank">DeviceNet</a> - CIP, not needed (hello my employer!)</li><li><a href="https://en.wikipedia.org/wiki/SAE_J1939" target="_blank">SAE_J1939</a> - too complex</li><li><a href="https://en.wikipedia.org/wiki/CANaerospace" target="_blank">CANaerospace</a> - too complex</li><li><a href="https://en.wikipedia.org/wiki/Cubesat_Space_Protocol" target="_blank">Cubesat_Space_Protocol</a> - better, good support, but excessive</li><li><a href="https://en.wikipedia.org/wiki/Very_Simple_Control_Protocol" target="_blank">Very_Simple_Control_Protocol</a> - not<strong>very</strong> simple</li></ul><h2 id="the-bicycle">The Bicycle</h2><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/Untitled%202.png" alt="Old Bicycle"/></p><p>It is turned out, that I don‚Äôt need to build a lot on top of bare CAN. The protocol is based on the usage of the ID field of the CAN frame. The field carries all the necessary information for the protocol. Here are some illustrations for the Standard and the Extended CAN frame (my know-how is in green):</p><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/canbus_frame_std.svg" alt="Standard Frame"/></p><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/canbus_frame_ext.svg" alt="Extended Frame"/></p><p>Each device has its own address, an ID. Information about the source, the destination, and the message type is embedded info the ID filed in a way that makes it simple to read by a human in HEX format. For example, if the device with the id #5 sends a message of type 1 to device #3, the message ID will be<strong>0x531</strong> and<strong>0x0503001</strong> for the standard and the extended frames respectively.</p><p>Since CAN bus devices by default are constantly sending data in the network I used it for device discovery. All devices should send a Presence message at least once in 3 seconds. If not, the device is considered disconnected.</p><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/Untitled%203.png" alt="Presence Messages"/></p><p>With such a message structure, it is very simple to address commands not only to a specific device but also to broadcast them using the destination of 0x0. Also sending data information can be broadcasted as well as dedicated to a specific device:</p><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/Untitled%204.png" alt="Commands"/></p><p>Commands</p><p><img class="img-zoomable" src="CAN%20bus%20and%20a%20new%20simple%20protocol%20b5025925c93f493abf97425c9879981d/Untitled%205.png" alt="Data Messages"/></p><p>Data Messages</p><p>The full protocol description is available on the Zakhar documentation page here:</p><p><a href="https://zakhar-the-robot.github.io/doc/docs/communication-protocols/canbus/" target="_blank">https://zakhar-the-robot.github.io/doc/docs/communication-protocols/canbus/</a></p><h2 id="whats-next">What‚Äôs Next?</h2><p>I‚Äôm finishing the firmware implementation for all my robot units. In the next article, I want to talk about the<strong>protocol implementation in the Zakhar the Robot project</strong>.</p><h2 id="nice-but-whats-about-the-alive-robot">Nice but What‚Äôs About the Alive Robot?</h2><p>The main purpose of the project is to investigate the potential of animal-like behavior for the human-robot interaction. It is a big sophisticated topic and working on this demands a reliable hardware basis. I plan to spend some more time working on this. Then I will return to the high-level problem. Also, as I am a low-level developer I know that the hardware architecture defines high-level possibilities so it is important to make it right. Thanks for your patience and stay tuned!</p><h2 id="links">Links</h2><ul><li><a href="https://blog.agramakov.me/" target="_blank">My blog</a></li><li><a href="https://zakhar-the-robot.github.io/doc/docs/communication-protocols/canbus/" target="_blank">Standard Documentation</a></li><li><a href="https://www.facebook.com/groups/zakhartherobot" target="_blank">Facebook</a></li><li><a href="https://www.instagram.com/zakhar_the_robot" target="_blank">Instagram</a></li><li><a href="https://hackaday.io/project/171888-zakhar-the-robot" target="_blank">Hackaday.io project page</a></li></ul></description></item><item><title>Zakhar II: Separately controllable DC motor speed</title><link>https://blog.agramakov.me/posts/2021/07-10-zakhar-ii-dc-speed/</link><pubDate>Sat, 10 Jul 2021 11:55:57 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/07-10-zakhar-ii-dc-speed/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/Oe909ac_hfE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Links:</p><ul><li><a href="http://zakhar.agramakov.me/" target="_blank">http://zakhar.agramakov.me/</a></li><li><a href="https://www.instagram.com/zakhar_the_robot/" target="_blank">https://www.instagram.com/zakhar_the_robot/</a></li></ul></description></item><item><title>"On the way to AliveOS" or "making of a robot with emotions is harder than I thought"</title><link>https://blog.agramakov.me/posts/2021/03-22-on-the-way-to-aliveos-or-making-of-a-robot-with-emotions-is-harder-than-i-thought/</link><pubDate>Mon, 22 Mar 2021 20:52:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/03-22-on-the-way-to-aliveos-or-making-of-a-robot-with-emotions-is-harder-than-i-thought/</guid><description><p>So, indeed, the more I develop the software for Zakhar, the more complicated it goes. So, first:</p><p><strong>Contributions and collaborations are welcome!</strong></p><p>If you want to participate, write to me and we will find what you can do in the project.</p><p>Second, feature branches got toooooo huge, so I&rsquo;ll use the workflow with the develop branch (<a href="https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow" target="_blank">Gitflow</a>) to accumulate less polished features and see some rhythmic development. Currently, I&rsquo;m actively (sometimes reluctantly) working on the integration of my<a href="https://github.com/an-dr/r_giskard_EmotionCore" target="_blank">EmotionCore</a> to the Zakhar&rsquo;s ROS system. So now, the last results are available in the<code>develop</code> branch:</p><p><a href="https://github.com/an-dr/zakharos_core/tree/develop" target="_blank">https://github.com/an-dr/zakharos_core/tree/develop</a></p><p>Third, to simplify the development of the robot&rsquo;s core software I would like to separate really new features from the implementation. So, Zakhar will be a demonstration platform and the mascot of the project&rsquo;s core. The core I&rsquo;d like to name the AliveOS, since implementation of the alive-like behavior is the main goal of the project. I am not sure entirely what the thing AliveOS will be. It is not an operating system so far, rather a framework, but I like the name :).</p><p>Fourth and the last. Usually, I write such posts after some accomplishment. That&rsquo;s true and today. I just merged the<a href="https://github.com/an-dr/zakharos_core/commit/0dedc96056a619cac51373bb9638a029522e7753" target="_blank">feature/emotion_core</a> branch to the develop one (see above). It is not a ready to use feature, but the core is working, getting affected by sensors and exchanging data with other nodes. It doesn&rsquo;t affect the behavior for now. For this I need to make some huge structural changes. The draft bellow (changes are in black and white)</p><p><img class="img-zoomable" src="2587901616447418757.png" alt=""/></p><p>Next steps:</p><ol><li>Implementing of the new architecture changes</li><li>Separation of the core packages into the one called AliveOS</li><li>Moving AliveOS to the separated repository and including it as a submodule (the repo already exists:<a href="https://github.com/an-dr/aliveos" target="_blank">https://github.com/an-dr/aliveos</a>)</li></ol><p>Thank you for reading! Stay tuned and participate!</p></description></item><item><title>EmotionCore - 1.0.0</title><link>https://blog.agramakov.me/posts/2021/02-23-emotioncore-1-0-0/</link><pubDate>Tue, 23 Feb 2021 19:59:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/02-23-emotioncore-1-0-0/</guid><description><p>First release!</p><p>The aim of this library is to implement an emotion model that could be used by other applications implementing behavior modifications (emotions) based on changing input data.</p><p>The Core has sets of<strong><strong><strong><strong>parameters</strong></strong></strong></strong>,<strong><strong><strong><strong>states</strong></strong></strong></strong> and<strong><strong><strong><strong>descriptors</strong></strong></strong></strong>:</p><p><strong><strong><strong><strong>Parameters</strong></strong></strong></strong> define the<strong><strong><strong><strong>state</strong></strong></strong></strong> of the core. Each state has a unique name (happy, sad, etc.)<strong><strong><strong><strong>Descriptors</strong></strong></strong></strong> specify the effect caused by input data to the parameters. The user can use either the state or the set of parameters as output data. Those effects can be:</p><ul><li>depending on sensor data</li><li>depending on time (temporary impacts)</li></ul><p>It is a cross-platform library used in the Zakhar project. You can use this library to implement sophisticated behavior of any of your device. Contribution and ideas are welcome!</p><p>Home page:<a href="https://github.com/an-dr/r_giskard_EmotionCore" target="_blank">r_giskard*_*EmotionCore: Implementing a model of emotions</a></p><p><img class="img-zoomable" src="534801614110287280.png" alt=""/></p></description></item><item><title>Updated draft of the ROS-node network with the Emotional Core</title><link>https://blog.agramakov.me/posts/2021/01-28-updated-draft-of-the-ros-node-network-with-the-emotional-core/</link><pubDate>Thu, 28 Jan 2021 15:25:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-28-updated-draft-of-the-ros-node-network-with-the-emotional-core/</guid><description><p>Working on the Emotion Core update it became clear to me that<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/188030-draft-of-the-updated-ros-node-network-with-the-emotional-core" target="_blank">placing responsibility of emotional analysis to Ego-like nodes</a> (a Consciousness, Reflexes, and Instincts) is a wrong approach. It leads to the situation where the developer of the application should specify several types of behavior themself, which is too complicated for development.</p><p>I want to implement another approach when concepts themselves contain information about how they should be modified based on a set of the emotion parameters. For example:</p><ol><li>An Ego-like node sends the concept<code>move</code> with a modifier<code>left</code>:</li></ol><div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-1"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-1">1</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-2"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-2">2</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-3"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-3">3</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-0-4"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-0-4">4</a></span></code></pre></td><td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"concept"</span>:<span style="color:#f1fa8c">"move"</span>,</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"modifier"</span>: [<span style="color:#f1fa8c">"left"</span>]</span></span><span style="display:flex;"><span>}</span></span></code></pre></td></tr></table></div></div><ol start="2"><li><p>The concept<code>move</code> contains a descriptor with information that: if adrenaline is lower than 5 - add a modifier<code>slower</code></p></li><li><p>The Concept-to-command interpreter update the concept to:</p></li></ol><div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-1"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-1">1</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-2"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-2">2</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-3"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-3">3</a></span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f" id="hl-1-4"><a style="outline:none;text-decoration:none;color:inherit" href="#hl-1-4">4</a></span></code></pre></td><td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"concept"</span>:<span style="color:#f1fa8c">"move"</span>,</span></span><span style="display:flex;"><span><span style="color:#ff79c6">"modifier"</span>: [<span style="color:#f1fa8c">"left"</span>,<span style="color:#f1fa8c">"slower"</span>]</span></span><span style="display:flex;"><span>}</span></span></code></pre></td></tr></table></div></div><ol start="4"><li>According the concept descriptors the Concept-to-command interpreter sends commands to the device Moving Platform:</li></ol><ul><li><a href="https://github.com/an-dr/zakhar_platform/blob/master/software/main/src/controlcallback.cpp#L36" target="_blank">Set speed to 2</a> (3 is maximum for the device)</li><li><a href="https://github.com/an-dr/zakhar_platform/blob/master/software/main/src/controlcallback.cpp#L29" target="_blank">Move left</a></li><li><a href="https://github.com/an-dr/zakhar_platform/blob/master/software/main/src/controlcallback.cpp#L37" target="_blank">Set speed to 3</a> (default value)</li></ul><p>In addition, I updated the diagram itself by structuring it and adding notes to make the entire system easier to understand. Here is the diagram:</p><p><img class="img-zoomable" src="1018361611848693962.png" alt=""/>
Now I will implement the structure above in the<a href="https://github.com/an-dr/zakharos_core/tree/feature/emotion_core" target="_blank">emotion_core</a> branch of the zakharos_core repository. The feature is getting closer to implementation. More updates soon.</p><p>Links:</p><p><a href="https://github.com/an-dr/zakharos_core/tree/feature/emotion_core" target="_blank">https://github.com/an-dr/zakharos_core/tree/feature/emotion_core</a></p><p><a href="https://github.com/an-dr/r_giskard_EmotionCore" target="_blank">https://github.com/an-dr/r_giskard_EmotionCore</a></p></description></item><item><title>Draft of the updated ROS-node network with the Emotional Core</title><link>https://blog.agramakov.me/posts/2021/01-12-draft-of-the-updated-ros-node-network-with-the-emotional-core/</link><pubDate>Tue, 12 Jan 2021 12:38:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-12-draft-of-the-updated-ros-node-network-with-the-emotional-core/</guid><description><p>Complexity of the networks is getting increased in a non-linear manner with any new type of node added, so documentation of the development is becoming more crucial than ever. So, I&rsquo;ve decided to, firstly redraw the network diagram to make it easier to read and contain more useful information. Then I&rsquo;ve spent some time on how the<a href="https://github.com/an-dr/r_giskard/tree/master/emotional_core" target="_blank">Emotional Core</a> should interact with other nodes.</p><p>Initially, I thought that naming each condition based on a set of emotional parameters is a clever idea. With that approach, we would only send the name of the emotion to the main program and that emotion would affect the robot behavior. But it leads to the situation when the robot has a set of discrete states, in other words, I would develop a pretty sophisticated state machine - and it is way far from how the animals behave.</p><p>Analyzing my feelings, I also cannot say that the name of emotions defines my behavior, I would say that it is rather an uninterrupted specter of states. So, it would be that the researches of<a href="https://en.wikipedia.org/wiki/Carroll_Izard" target="_blank">Carroll Izard</a> and his colleagues in distinguishing human emotions I&rsquo;ve read a lot last months are not applicable to my project. As well as the part of the Emotional Core responsible for changing emotions depending on a set of emotional parameters. A bit sad about spent time, but it is the development process.</p><p>So now I have a draft of the structure I will use integrating the Emotional Core into the mind of Zakhar.</p><p><img class="img-zoomable" src="4854811610453051265.png" alt=""/></p></description></item><item><title>Update of the ROS-network</title><link>https://blog.agramakov.me/posts/2021/01-10-update-of-the-ros-network/</link><pubDate>Sun, 10 Jan 2021 21:32:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-10-update-of-the-ros-network/</guid><description><p>Hi! I merged a huge update for<a href="https://github.com/an-dr/zakharos_core" target="_blank">zakharos_core</a> - the main part of the Zakhar project.</p><p>The repository is a<a href="http://wiki.ros.org/" target="_blank">Robot Operating System</a> network where the main application consists of ego-like nodes: the consciousness (the main application) and instincts (interruptions). Each of them operates with concepts in the manner as our mind works.</p><p>As I already said, my aim is to make a robot that behaves like an animal, and hence having more understandable for the user behavior.</p><p>After this update the next step is to integrate my already developed<a href="https://github.com/an-dr/r_giskard/tree/master/emotional_core" target="_blank">emotional core</a> , which is basically an attempt to recreate the endocrine system of alive organisms.</p><p>Returning to the update. Currently the robot uses three ego-like nodes:</p><ul><li>node-consciousness: the Small researcher - robot is moving by circles.</li><li>instinct: Bird Panic - analyzing light changing patterns to recognizing a single moving fast shadow, then put the robot in the panic state, when it is trying to find a darker (safer) place.</li><li>instinct: Avoid Close Objects - every object closer then 5 cm should be avoided. Object in front leads to moving backward. Objects at sides activate the algorithm: move back, turn on 60 degrees.</li></ul><p>The ROS-network is shown on the picture bellow. Also have a look at the robot in action on a video. Thanks!</p><p><img class="img-zoomable" src="4782381610313730599.png" alt=""/></p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/syaosnG6t_o" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>Hardware structure</title><link>https://blog.agramakov.me/posts/2021/01-09-hardware-structure/</link><pubDate>Sat, 09 Jan 2021 16:15:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-09-hardware-structure/</guid><description><p>Just added illustrations describing hardware used in the project to the repository:</p><p><a href="https://github.com/an-dr/zakhar/tree/ec7c4e69af0f5b44aa3dde2fd7766416ac783668" target="_blank">https://github.com/an-dr/zakhar/tree/ec7c4e69af0f5b44aa3dde2fd7766416ac783668</a></p><p>Here are the pictures:</p><p><img class="img-zoomable" src="4001381610208793411.png" alt=""/></p><p>And detailed for each device:</p><p><img class="img-zoomable" src="4956611610208844010.png" alt=""/><img class="img-zoomable" src="9870381610208816820.png" alt=""/><img class="img-zoomable" src="4438441610208825001.png" alt=""/><img class="img-zoomable" src="1942821610208834869.png" alt=""/></p></description></item><item><title>First test of the second instinct for Zakhar</title><link>https://blog.agramakov.me/posts/2021/01-08-first-test-of-the-second-instinct-for-zakhar/</link><pubDate>Fri, 08 Jan 2021 23:13:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-08-first-test-of-the-second-instinct-for-zakhar/</guid><description><p>Testing of the mind of Zakhar with two &ldquo;instincts&rdquo;:</p><ol><li><p>If a bird-like shadow above him, he expresses anxious and hides</p></li><li><p>If any obstacle in front - move back</p></li></ol><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/UETOz4RXgqY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>New test site!</title><link>https://blog.agramakov.me/posts/2021/01-07-new-test-site/</link><pubDate>Thu, 07 Jan 2021 20:44:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2021/01-07-new-test-site/</guid><description><p>Just finished a new site for tests and demonstrations!</p><p><img class="img-zoomable" src="9955191610052220783.jpeg" alt=""/></p><p>New place, new hardware, more updates soon!</p></description></item><item><title>Three Ultrasound Sensors</title><link>https://blog.agramakov.me/posts/2020/12-17-three-ultrasound-sensors/</link><pubDate>Thu, 17 Dec 2020 20:02:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/12-17-three-ultrasound-sensors/</guid><description><p>Unfortunately, I broke my ultrasound sensor during resoldering of connectors. Taking advantage of necessity of buying something, I&rsquo;ve bought three new ones!</p><p><img class="img-zoomable" src="6016331608235294592.jpg" alt=""/>
Updated code with a new ultrasound sensor driver:</p><p><a href="https://github.com/an-dr/zakhar_sensors/tree/c4e7a5e5c91acbbc1efdaaa4122e46428793c973" target="_blank">https://github.com/an-dr/zakhar_sensors/tree/c4e7a5e5c91acbbc1efdaaa4122e46428793c973</a></p></description></item><item><title>Sensor Platform with STM32</title><link>https://blog.agramakov.me/posts/2020/12-07-sensor-platform-with-stm32/</link><pubDate>Mon, 07 Dec 2020 20:10:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/12-07-sensor-platform-with-stm32/</guid><description><p>Done! Now sensors data is collecting by a powerful STM32 MCU with FreeRTOS. This will help adding even more sensors. For now, there are two:</p><ul><li><a href="https://www.sparkfun.com/products/15569" target="_blank">Ultrasonic Distance Sensor - HC-SR04</a></li><li><a href="https://arduinomodules.info/ky-018-photoresistor-module/" target="_blank">KY-018 Photoresistor Module</a></li></ul><p>Here is a video of the update in action:</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/L2-5uH89-tY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Links:</p><ul><li><a href="https://github.com/an-dr/zakhar_sensors/tree/ebcc7f652904122da11c9facf0f1f92af8f6b9fe" target="_blank">Updated source code with STM32 support</a></li><li><a href="https://github.com/an-dr/SharedVirtualRegisters" target="_blank">Repository of my SharedVirtualRegisters library that I used in the update</a> - it is thread-safe and supports FreeRTOS, but can work without any OS as well</li><li><a href="https://github.com/an-dr/log.cx" target="_blank">Repository of the logging library</a> - it is a fork of the<a href="https://github.com/rxi/log.c" target="_blank">log.c library</a> by rxi.</li></ul><p>Photos bellow.</p><p><img class="img-zoomable" src="3319311607372116905.jpg" alt=""/><img class="img-zoomable" src="5859161607372162135.jpg" alt=""/><img class="img-zoomable" src="6079091607371932650.jpg" alt=""/><img class="img-zoomable" src="9371511607372071739.jpg" alt=""/></p></description></item><item><title>Calm prototyping evening</title><link>https://blog.agramakov.me/posts/2020/11-16-calm-prototyping-evening/</link><pubDate>Mon, 16 Nov 2020 20:03:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/11-16-calm-prototyping-evening/</guid><description><p>It&rsquo;s a good night to move the sensor platform to a new MCU and add some sensors to demonstrate working of the<a href="https://github.com/an-dr/r_giskard/tree/master/emotional_core" target="_blank">emotional core</a> (<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">roadmap</a>).</p><p>PR for the update (WIP):<a href="https://github.com/an-dr/zakhar_sensors/pull/4" target="_blank">https://github.com/an-dr/zakhar_sensors/pull/4</a></p><p><img class="img-zoomable" src="6458291605556935143.jpg" alt=""/><img class="img-zoomable" src="3454001605556945627.png" alt=""/></p></description></item><item><title>Emotional Core: temp impacts + refactoring</title><link>https://blog.agramakov.me/posts/2020/10-28-emotional-core-temp-impacts-refactoring/</link><pubDate>Wed, 28 Oct 2020 15:04:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/10-28-emotional-core-temp-impacts-refactoring/</guid><description><p>Just finished a huge update for the Core. New feature is temporary impacts. It simulates a situation, that you can get watching<a href="https://www.youtube.com/watch?v=BpmTtWC5BJU" target="_blank">a screamer</a> (Caution! ). Something scarry is happening, you are getting a lot of adrenaline and cortisol, after 5 minutes you are ok.</p><p>Also, the structure of the core was dramatically simplified. On the picture temporary impacts are called Conscious Data because I&rsquo;m going to use it to simulate &ldquo;bad thoughts&rdquo; of the robot.</p><p><img class="img-zoomable" src="4452771603897229885.png" alt=""/>
Update is here (there is an example you can try :) ):</p><p><a href="https://github.com/an-dr/r_giskard/tree/4e7e5115e7975b12e065ffd98130c0e9ba97f551/emotional_core" target="_blank">https://github.com/an-dr/r_giskard/tree/4e7e5115e7975b12e065ffd98130c0e9ba97f551/emotional_core</a></p></description></item><item><title>Emotional Core: in details</title><link>https://blog.agramakov.me/posts/2020/10-16-emotional-core-in-details/</link><pubDate>Fri, 16 Oct 2020 21:06:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/10-16-emotional-core-in-details/</guid><description><p>Now after some polishing of the basic emotional core I can tell about how it works and about it&rsquo;s structure.</p><p>Emotional core consists of three main parts (see the picture bellow):</p><ul><li>Input Data Descriptors - describes data from sensors and how it should affect the core</li><li>Emotional States Descriptors - named states of the core described by specific values of core parameters</li><li>Core State - contains core parameters, value of sensors and pointer to relevant to parameters Emotional States Descriptors</li></ul><p><img class="img-zoomable" src="5680781602881967541.png" alt=""/>
Or more detailed:</p><p><img class="img-zoomable" src="6296431602882202035.png" alt=""/>
When you write the data, the core does the following:</p><ul><li>updates saved sensor value</li><li>updates core parameter: param += (new_sens_val - old_sens_val) * weight</li><li>updates the current core state based on updated parameters</li></ul><p>Here are some examples of data that could be used with the core.</p><p>Example of core parameters:</p><p>[ &ldquo;cortisol&rdquo;,</p><p>&ldquo;dopamine&rdquo;,</p><p>&ldquo;adrenaline&rdquo;,</p><p>&ldquo;serotonin&rdquo; ]
Example of a core&rsquo;s emotional state:</p><p>{</p><p>&ldquo;name&rdquo;: happiness,</p><p>&ldquo;conditions&rdquo;: [</p><p>{</p><p>&ldquo;param&rdquo;: &ldquo;cortisol&rdquo;,</p><p>&ldquo;op&rdquo;: LESS_THAN,</p><p>&ldquo;value&rdquo;: 10</p><p>},</p><p>{</p><p>&ldquo;param&rdquo;: &ldquo;serotonin&rdquo;,</p><p>&ldquo;op&rdquo;: GREATER_THAN,</p><p>&ldquo;value&rdquo;: 100</p><p>}</p><p>]</p><p>}
Example of input data descriptor:</p><p>{</p><p>&ldquo;sensor_name&rdquo;: &ldquo;temperature sensor&rdquo;,</p><p>&ldquo;val_min&rdquo;: 0,</p><p>&ldquo;val_max&rdquo;: 255,</p><p>&ldquo;weights&rdquo;: [</p><p>{</p><p>&ldquo;core_param_name&rdquo;: &ldquo;serotonin&rdquo;,</p><p>&ldquo;weight&rdquo;: 0.5</p><p>},</p><p>{</p><p>&ldquo;core_param_name&rdquo;: &ldquo;cortisol&rdquo;,</p><p>&ldquo;weight&rdquo;: -0.5</p><p>}</p><p>]</p><p>}
Input data example:</p><p>{</p><p>&ldquo;sensor_name&rdquo;: &ldquo;temperature sensor&rdquo;,</p><p>&ldquo;value&rdquo;: 120</p><p>}
My next step will be in looking appropriate parameters and weights that can adequately describe (roughly of course) some human emotions.</p><p>Source core:<a href="https://github.com/an-dr/r_giskard" target="_blank">https://github.com/an-dr/r_giskard</a></p></description></item><item><title>The Emotional Core Test (attention: pretty boring!)</title><link>https://blog.agramakov.me/posts/2020/09-26-the-emotional-core-test-attention-pretty-boring/</link><pubDate>Sat, 26 Sep 2020 23:02:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/09-26-the-emotional-core-test-attention-pretty-boring/</guid><description><p>Testing the new Emotional Core for the R.Giskard project</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/YS33SozeLH4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>The code is here:<a href="https://github.com/an-dr/r_giskard/tree/feature/emotional_core" target="_blank">https://github.com/an-dr/r_giskard/tree/feature/emotional_core</a></p></description></item><item><title>Emotional Core Sketch</title><link>https://blog.agramakov.me/posts/2020/09-19-emotional-core-sketch/</link><pubDate>Sat, 19 Sep 2020 18:24:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/09-19-emotional-core-sketch/</guid><description><p>Well,<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">the third milestone is in progress</a>.</p><p>It&rsquo;s time for the most interesting part of the project - emotions and reflexes. Now, let&rsquo;s talk about emotions, but in short.</p><p>In my view and my understanding of what should be going on here, emotions - it is just the name of our mental state. We are going to have a set of such emotional states - to use those states as a simple indicator for our systems using my emotions implementation.</p><p>In the animal case, each state is defined by the specific chemical condition of the brain. The virtual analog of those chemical components probably should be a set of parameters.</p><p>Input sensorial data could affect the parameters with different weights. E.g. the huger amount of light should decrease the parameter reflecting the level of anxiety.
If the level of anxiety is low enough, we could declare that the code in a calm state.</p><p>The pretty rough structure of the emotional core is on the picture:</p><p><img class="img-zoomable" src="4753741600626101826.png" alt=""/>
The work is going at the branch of my r_giskard repository, where I&rsquo;m going to implement and the core and the simulator to test it on desktop systems:</p><p><a href="https://github.com/an-dr/r_giskard/tree/feature/simulator" target="_blank">https://github.com/an-dr/r_giskard/tree/feature/simulator</a></p><p>I&rsquo;m implementing it in C++ because in that scenario the core would be used in embedded systems as well as in python programs.</p></description></item><item><title>Zakhar Milestone: Zakharos</title><link>https://blog.agramakov.me/posts/2020/08-29-zakhar-milestone-zakharos/</link><pubDate>Sat, 29 Aug 2020 21:01:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-29-zakhar-milestone-zakharos/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/CtFNXtHARPk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Changelog since the<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/178126-the-reptile-demo" target="_blank">Reptile Demo</a> Milestone:</p><p><strong>Common:</strong></p><ul><li>Updated cable system with 8-pin connectors (see:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/181955-power-post-part-one" target="_blank">Power post. Part one.</a>).</li><li>Added union connectors for charging of all platforms at once.</li><li>Added switchers for each platform.</li><li>Created a new repository including the sensor platform and the face module:<a href="https://github.com/an-dr/zakhar_io" target="_blank">zakhar_io</a></li></ul><p><strong>Sensor platform:</strong></p><ul><li>Used Arduino Nano v3 instead Pro Micro - for better stability</li><li>United with face module as one platform</li><li>Added Ultrasonic Sensor HC-SR04, but without software support yet.</li><li>Updated face module with stabilized voltage</li></ul><p><strong>Computing (brain) platform:</strong></p><ul><li>Raspberry Pi 4, ROS Noetic with Python 3</li><li>New<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179474-improved-ros-based-architecture-for-the-program-core" target="_blank">ROS-based application architecture</a></li><li>OLED as status display for the computing platform (see:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/182936-startup-check" target="_blank">Startup check</a>)</li><li>Achieved stable (finally) I2C connection. Thanks to lower frequency (10KHz) and the new connectors.</li><li>Added Raspberry Pi Camera, but without software support yet.</li><li>Created a new repository for the platform:<a href="https://github.com/an-dr/zakhar_brain" target="_blank">an-dr/zakhar_brain: Software for Zakhar&rsquo;s brain</a></li></ul><p><strong>Moving platform:</strong></p><ul><li>New chassis with a stand</li><li>ESP32 instead of Arduino Nano v3</li><li>Bluetooth connection control</li><li>Position module MPU9250 with ability to turn for a given angle (see:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/180662-new-esp32-based-platform-testing-angles" target="_blank">New ESP32-based platform testing: Angles!</a>)</li><li>New third wheel - with rubber</li><li>Removed rotation encoders as useless</li><li>Updated motors by the modification with a metal reductor</li></ul><p>Repository:<a href="https://github.com/an-dr/zakhar" target="_blank">https://github.com/an-dr/zakhar</a></p></description></item><item><title>Startup check</title><link>https://blog.agramakov.me/posts/2020/08-29-startup-check/</link><pubDate>Sat, 29 Aug 2020 19:42:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-29-startup-check/</guid><description><p>The startup system check is shown on an OLED display. The display has its separated i2c bus (I2C-3) to communicate with the Raspberry.</p><p>The startup process is the following:</p><ul><li>Greeting</li><li>Test I2C-1 devices: moving platform, sensor platform, and face module. If something is not represented on the bus, the error will be shown.</li><li>Waiting while ssh service will be loaded</li><li>Waiting while the network will be connected</li><li>Waiting while roscore will be launched</li><li>Infinite loop showing the network and project info. The robot is ready!</li></ul><p>Gif of the process:</p><p><img class="img-zoomable" src="https://media4.giphy.com/media/Ie4Qa31pFDiuYpS7Fg/giphy.gif" alt=""/>
Code of the startup check process:</p><p><a href="https://github.com/an-dr/zakhar_service/tree/feature/display_n_startup/display" target="_blank">https://github.com/an-dr/zakhar_service/tree/feature/display_n_startup/display</a></p></description></item><item><title>Noctural</title><link>https://blog.agramakov.me/posts/2020/08-15-noctural/</link><pubDate>Sat, 15 Aug 2020 23:34:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-15-noctural/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/gB2MD0bJi-k" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>[Bad Idea, canceled] Power post. Part one.</title><link>https://blog.agramakov.me/posts/2020/08-08-bad-idea-canceled-power-post-part-one/</link><pubDate>Sat, 08 Aug 2020 15:14:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/08-08-bad-idea-canceled-power-post-part-one/</guid><description><p><img class="img-zoomable" src="5120251596899760195.png" alt=""/></p><p>There are a lot of issues when you connect low- and high-current devices to the same electrical circuit. Basically, you are getting:</p><ul><li><p>Noise at sensors&rsquo; output</p></li><li><p>Freezes and resets of MCUs</p></li><li><p>Artefacts on LCD and backlight&rsquo;s flashing</p></li><li><p>Other unexpected issues</p></li></ul><p>That&rsquo;s really annoying that&rsquo;s why I spend last several weeks thinking how to implement power supply for Zakhar. There are two things I want from this update: to avoid things mentioned above and to simplify development allowing me to work with every platform separately. There is also thing that I don&rsquo;t want, to develop a reliable power system. Please get me right, it is cool and interesting, but it is not something new. So, what I want is:</p><ul><li><p>Use as many ready-to-use components</p></li><li><p>Make an isolated power supply system for each platform</p></li></ul><p>Sounds like power banks!</p><p>I&rsquo;ve been already using a cool power bank with two low- and high-current ports for motors and the rest electronics:</p><p><img class="img-zoomable" src="8165941596898390849.jpg" alt=""/>
I will use it for the moving platform only.</p><p>One months ago, I&rsquo;ve already added another one for the computing platform with a Raspberry:</p><p><img class="img-zoomable" src="4828081596898493713.jpg" alt=""/>
Since the face module and the sensor platform haven&rsquo;t been separated almost never during the development, I decided to add to them only single common power bank. Unfortunately, I haven&rsquo;t found anything, so I had to make own one:</p><p><img class="img-zoomable" src="9793171596898834447.jpg" alt=""/>
(I know, LEGO AGAIN!)</p><p>Okey. Now I must charge 3 power banks. Easy! Since we are not interested of using four pins of inter-platform connectors for power delivery anymore it&rsquo;s time to update the connectors for delivery of charge to power banks.</p><p>Again, to not spending the time for development of physical connectors, let&rsquo;s use best practices.</p><ol><li>Take breadboards:</li></ol><p><img class="img-zoomable" src="3939441596899318766.jpg" alt=""/>
2. Think a little</p><p><img class="img-zoomable" src="5444821596899369366.png" alt=""/>
3. Cut!</p><p><img class="img-zoomable" src="2719511596899494609.jpg" alt=""/>
And here what and how we can connect:</p><p><img class="img-zoomable" src="184951596899577714.png" alt=""/>
As a result, two platforms connected look like:</p><p><img class="img-zoomable" src="5555441596899880139.jpg" alt=""/>
And no more blinking of LCD backlight while the Raspberry is thinking, or motors are turning!</p><p>I&rsquo;m waiting a bunch of cables and switchers so next time I&rsquo;ll show the final variant of Zakhar&rsquo;s power supply solution and we, having a stable and plug-and-play set of flatform will continue to investigate the life behavior and how to implement it for the robot!</p><p>Stay tuned!</p><p>P.S If you have any better solutions let me know!</p></description></item><item><title>Turns: -45, +45, -90, +90 degrees</title><link>https://blog.agramakov.me/posts/2020/07-21-turns-45-45-90-90-degrees/</link><pubDate>Tue, 21 Jul 2020 17:07:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-21-turns-45-45-90-90-degrees/</guid><description><p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/Raywkmo5Lf8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div>
Updated code here:<a href="https://github.com/an-dr/zakhar_platform/pull/2" target="_blank">https://github.com/an-dr/zakhar_platform/pull/2</a></p></description></item><item><title>Assembled with the new moving platform! Turn 90 degree</title><link>https://blog.agramakov.me/posts/2020/07-19-assembled-with-the-new-moving-platform-turn-90-degree/</link><pubDate>Sun, 19 Jul 2020 18:43:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-19-assembled-with-the-new-moving-platform-turn-90-degree/</guid><description><p>From the Raspberry the moving platform gets an argument 0x5a (90) degrees, then a command to turn right. If the argument is 0x00 command is executing during 100ms then the platform stops.</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/_nnMFdBr9SM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>New ESP32-based platform testing: Angles!</title><link>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-angles/</link><pubDate>Sat, 11 Jul 2020 18:20:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-angles/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/Z1AaA5RbPfY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Works:</p><ul><li>GY-91 through I2C: gyroscope + accelerometer</li><li>Serial for external control</li><li>I2C slave for external control</li><li>Wireless connection - Bluetooth serial port</li><li>Motors control</li><li>PWM for motors - three speeds</li><li>RPM encoders</li><li>Position data handling - calibration, filtering, conversion in angles</li></ul><p>To-do:</p><ul><li>Code refactoring and configuration with<a href="https://en.wikipedia.org/wiki/Menuconfig" target="_blank">menuconfig</a></li></ul><p>Work in progress here:<a href="https://github.com/an-dr/zakhar_platform/pull/1" target="_blank">Feature/esp32 by an-dr Pull Request #1 an-dr/zakhar_platform</a></p></description></item><item><title>New ESP32-based platform testing: Updates!</title><link>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-updates/</link><pubDate>Sat, 11 Jul 2020 13:12:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-11-new-esp32-based-platform-testing-updates/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/SugTDM9RSro" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Works:</p><ul><li>GY-91 through I2C: gyroscope + accelerometer</li><li>Serial for external control</li><li>I2C slave for external control</li><li>Wireless connection - Bluetooth serial port</li><li>Motors control</li><li>PWM for motors - three speeds</li><li>RPM counts. (BTW the motors have different speed as it turned out )</li></ul><p>To-do:</p><ul><li>Position data handling - calibration, filtering, conversion in angles (?)</li><li>Code refactoring and configuration with<a href="https://en.wikipedia.org/wiki/Menuconfig" target="_blank">menuconfig</a></li></ul><p>Work in progress here:<a href="https://github.com/an-dr/zakhar_platform/pull/1" target="_blank">Feature/esp32 by an-dr Pull Request #1 an-dr/zakhar_platform</a></p></description></item><item><title>New ESP32-based platform testing</title><link>https://blog.agramakov.me/posts/2020/07-10-new-esp32-based-platform-testing/</link><pubDate>Fri, 10 Jul 2020 18:03:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-10-new-esp32-based-platform-testing/</guid><description><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/6LgkI2rKElI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div><p>Works:</p><ul><li>GY-91 through I2C: gyroscope + accelerometer</li><li>Serial for external control</li><li>I2C slave for external control</li><li>Motors control</li></ul><p>To-do:</p><ul><li>Wireless connection</li><li>Position data handling - calibration, filtering, conversion in angles (?)</li><li>PWM for motors GPIO</li><li>RPM counts</li></ul><p>Work in progress here:<a href="https://github.com/an-dr/zakhar_platform/pull/1" target="_blank">Feature/esp32 by an-dr Pull Request #1 an-dr/zakhar_platform</a></p></description></item><item><title>Zakhar disassembled</title><link>https://blog.agramakov.me/posts/2020/07-06-zakhar-disassembled/</link><pubDate>Mon, 06 Jul 2020 21:04:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-06-zakhar-disassembled/</guid><description><p>While the robot is changing all the time, after the last update I&rsquo;ve decided to take some snapshot of its current state and show to the public what&rsquo;s going on now. Let&rsquo;s start:</p><p><img class="img-zoomable" src="9968151594067608724.png" alt=""/>
Zakhar consists of three (sometimes four) platforms and a service OLED display. I&rsquo;m using the last one for debugging, testing, etc.</p><p><strong>Moving platform</strong></p><p>The moving platform just got the update with ESP32-board as a replacement for old Arduino and with the MPU module</p><p><img class="img-zoomable" src="8897821594067828971.png" alt=""/>
I did it for the sake of better multitasking which is needed to me for simpler handling of data from sensors and commands from the computing platform.</p><p>The platform is carrying on a power bank which supplies motors of the platform and all other electronics of Zakhar (except Raspberry Pi).</p><p>The power, as well as commands, are transmitting through unified 8-pin connectors (2 pins for I2C, 2 pins for 5v of low current, 2 pins for 5v of high current, 2 pins - reserved)</p><p><strong>Computing platform</strong></p><p>The computing platform has Raspberry Pi 4B with 4Gb of RAM and it&rsquo;s own power bank - for a stable supply of the Raspberry.</p><p><img class="img-zoomable" src="7773861594068663569.png" alt=""/><strong>Face+sensor platform</strong></p><p>The platform is carrying an Arduino which performs as an I2C device with data collected from sensors (i have only one now) into I2C registers and the face module.</p><p><img class="img-zoomable" src="9146271594068881703.png" alt=""/>
Inside the face module, there are another Arduino and a DC-DC convertor (for LCD supplying)</p><p><img class="img-zoomable" src="7523901594069285373.jpg" alt=""/><img class="img-zoomable" src="1412781594069239376.png" alt=""/>
That&rsquo;s it, thank you for your attention!</p><p>I&rsquo;m going back to writing the firmware for ESP32. CU soon!</p></description></item><item><title>Prototype of a new moving platform. Work in progress</title><link>https://blog.agramakov.me/posts/2020/07-03-prototype-of-a-new-moving-platform-work-in-progress/</link><pubDate>Fri, 03 Jul 2020 22:28:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/07-03-prototype-of-a-new-moving-platform-work-in-progress/</guid><description><p>Planned: ESP32, gyroscope, accelerometer, onboard data processing, variable speed, wireless mode and more!</p><p><img class="img-zoomable" src="255951593815161439.jpeg" alt=""/><a href="https://github.com/an-dr/zakhar_platform/tree/feature/esp32" target="_blank">https://github.com/an-dr/zakhar_platform/tree/feature/esp32</a></p></description></item><item><title>Some zakharos_core documentation</title><link>https://blog.agramakov.me/posts/2020/06-30-some-zakharos_core-documentation/</link><pubDate>Tue, 30 Jun 2020 18:30:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-30-some-zakharos_core-documentation/</guid><description><p>Before moving to the reimplementing of moving platform on ESP32 I&rsquo;ve added some documentation to the<a href="https://github.com/an-dr/zakharos_core" target="_blank">zakharos_core repository</a>. Code has a lot of roughness that will be polished later (probably on the next milestone -<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">The Emotions Demo</a>)</p><p>Documentation is not complete but will be supplemented with time. Probably the illustration, requirements and how to build should be enough to check the implementation out. So PTAL if you are interested!</p><p>Repository:<a href="https://github.com/an-dr/zakharos_core" target="_blank">an-dr/zakharos_core</a></p><p>The code inaction:<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179932-reimplemented-the-reptile-demo-with-ros" target="_blank">Reimplemented the Reptile demo with ROS!</a></p><p><img class="img-zoomable" src="3999271593540979159.f1c5e4f228a0765ffc63961102342a7a" alt=""/></p></description></item><item><title>Reimplemented the Reptile demo with ROS!</title><link>https://blog.agramakov.me/posts/2020/06-29-reimplemented-the-reptile-demo-with-ros/</link><pubDate>Mon, 29 Jun 2020 20:37:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-29-reimplemented-the-reptile-demo-with-ros/</guid><description><p>Half-milestone. What&rsquo;s new for now:</p><ul><li>Brand new moving platform</li><li>United sensor/face platform</li><li>Raspberry Pi 4</li><li>New<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179474-improved-ros-based-architecture-for-the-program-core" target="_blank">ROS-based application architecture</a></li><li>Updated face module with stabilized voltage</li><li>Updated cable system with 8-pin connectors</li><li>Stable (finally) I2C connection. Thanks to lower frequency (10KHz) and the new connectors.</li></ul><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/FIUM3jjPLFA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>Out of memory: moving to RPi4</title><link>https://blog.agramakov.me/posts/2020/06-25-out-of-memory-moving-to-rpi4/</link><pubDate>Thu, 25 Jun 2020 17:59:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-25-out-of-memory-moving-to-rpi4/</guid><description><p>Ok,<a href="https://www.ros.org/" target="_blank">ROS</a> needs RAM.</p><p>Just received Raspberry Pi 4 B with 4 GB of RAM and finally I can continue implementation of<a href="https://hackaday.io/project/171888-zakhar-the-robot/log/179474-improved-ros-based-architecture-for-the-program-core" target="_blank">this structure</a>. Stay tuned!</p><p><img class="img-zoomable" src="2743011593107962568.jpg" alt=""/></p></description></item><item><title>Improved ROS-based architecture for the program core</title><link>https://blog.agramakov.me/posts/2020/06-18-improved-ros-based-architecture-for-the-program-core/</link><pubDate>Thu, 18 Jun 2020 21:34:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-18-improved-ros-based-architecture-for-the-program-core/</guid><description><p>I&rsquo;m about to finish migration to the ROS. Aside small things, architecture seems crystalized. It looks like following:</p><p><img class="img-zoomable" src="344831592515514675.png" alt=""/>
Every block represents ROS-node (except the hardware block). Interaction between blocks is trying to mimic a real brain (as I understand what is happening there).</p><p>I&rsquo;m lazy to describe every node right now, I will write an article when this work will end up with the<a href="https://github.com/an-dr/zakhar#milestones" target="_blank">second demo</a>. But if you interested you can check the code out in the repository here:</p><p><a href="https://github.com/an-dr/zakharos_core" target="_blank">https://github.com/an-dr/zakharos_core</a></p><p>More about ROS:<a href="http://wiki.ros.org/" target="_blank">http://wiki.ros.org/</a></p></description></item><item><title>Just a photo with current state and a repo I'm working on.</title><link>https://blog.agramakov.me/posts/2020/06-11-just-a-photo-with-current-state-and-a-repo-im-working-on/</link><pubDate>Thu, 11 Jun 2020 19:03:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-11-just-a-photo-with-current-state-and-a-repo-im-working-on/</guid><description><p><img class="img-zoomable" src="2648581591903542559.jpg" alt=""/><a href="https://github.com/an-dr/zakharos_core" target="_blank">https://github.com/an-dr/zakharos_core</a> (just pushed one more step according<a href="https://github.com/an-dr/zakhar/blob/master/README.md#milestones" target="_blank">the plan</a>)</p></description></item><item><title>Moving platform update 1</title><link>https://blog.agramakov.me/posts/2020/06-06-moving-platform-update-1/</link><pubDate>Sat, 06 Jun 2020 08:44:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/06-06-moving-platform-update-1/</guid><description><p>When three wheels are better than four</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/inOzUCIyt_U" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>Thoughts about execution of commands in a mind-like program</title><link>https://blog.agramakov.me/posts/2020/05-31-thoughts-about-execution-of-commands-in-a-mind-like-program/</link><pubDate>Sun, 31 May 2020 20:36:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-31-thoughts-about-execution-of-commands-in-a-mind-like-program/</guid><description><p>I&rsquo;m trying to reverse engineer myself and apply it to the Zakhar. Some points:</p><ol><li>I can&rsquo;t send a direct order to, let say, my hand</li><li>I must form a concept, like &ldquo;fold fingers into a fist&rdquo; or &ldquo;move the hand up&rdquo;</li></ol><p>What that&rsquo;s mean for Zakhar:</p><ul><li>The main program should use high-level commands or concept</li><li>It is needed one more node translating these concepts into commands for devices</li></ul><p>Possible in-program interaction using terms of ROS and Zakhar should be like:</p><p><img class="img-zoomable" src="3901601590956653230.jpg" alt=""/>
The thoughts are related to:<a href="https://github.com/an-dr/zakharos_core" target="_blank">https://github.com/an-dr/zakharos_core</a></p></description></item><item><title>The ‚ÄãZakharos Milestone</title><link>https://blog.agramakov.me/posts/2020/05-31-the-zakharos-milestone/</link><pubDate>Sun, 31 May 2020 09:46:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-31-the-zakharos-milestone/</guid><description><p>Started the work on the next milestone called Zakharos. Aims of the milestone:</p><ul><li>Re-implementing The Reptile Demo with<a href="https://www.ros.org/" target="_blank">ROS</a> - for simpler development of sophisticated Mind&rsquo;s structures</li><li>Re-implementing Moving platform with the ESP32 , an accelerometer, PWM, FreeRTOS and WIFI communication - for faster responses, precise positioning, simpler access and more stable working.</li></ul><p><a href="https://github.com/an-dr/zakhar#milestones" target="_blank">Project&rsquo;s milestones</a></p></description></item><item><title>Robot with the Conscious: Imitating animal behavior for reducing user's anxiety</title><link>https://blog.agramakov.me/posts/2020/05-29-robot-with-the-conscious-imitating-animal-behavior-for-reducing-users-anxiety/</link><pubDate>Fri, 29 May 2020 18:59:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-29-robot-with-the-conscious-imitating-animal-behavior-for-reducing-users-anxiety/</guid><description><p>Finally! Just finished an article about my robot.</p><p><a href="https://blog.agramakov.me/2020/05/29/robot-with-the-conscious/" target="_blank">Robot with the Conscious: Imitating animal behavior for reducing user s anxiety</a></p><p>#robot #article #robotics #embedded #zakhar #hardware #automation #engineering #hobby #diyproject #embeddedsystems #electronics</p><p><img class="img-zoomable" src="9575731590778747528.png" alt=""/></p></description></item><item><title>Robot with the Conscious: Imitating animal behavior for reducing user's anxiety</title><link>https://blog.agramakov.me/posts/2020-05-29-robot-with-the-conscious/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>https://blog.agramakov.me/posts/2020-05-29-robot-with-the-conscious/</guid><description><p>Author:<a href="https://agramakov.me" target="_blank">Andrei Gramakov</a></p><h1 id="images6gif"><img class="img-zoomable" src="images/6.gif" alt=""/></h1><h2 id="the-problem">The Problem</h2><p>Nowadays, an increasing number of people is facing technophobia, a fear of technology. There are many possible causes for this, such as the following:</p><ul><li>Possibility of technical product damage, not understanding the correct treatment</li><li>Lack of experience</li><li>Fear of changes [1]</li><li>Effect of uncanny valley [2]</li></ul><p>All these causes are inherent to human mind, but possibly can be solved or bypassed.</p><h2 id="the-solution">The Solution</h2><p>The most comprehensive response to the problem, as it is seen by me, is to make a robot more resembling to those type of things with which we¬† interact all our lives: to other alive creatures, ourselves, and our pets. In other words, I want to naturalize the robot behavior. How would it solve the problem?</p><ul><li>As the main purpose of any living creature is to keep itself alive, most of the developed animals have a system of signals, telling other individuals what kind of interaction is acceptable is and what kind is not. These signals are basically emotions and reflexes. And this way I suggest telling user to handle the robot properly.</li><li>These hints should fill the gaps in the user‚Äôs experience, in the most natural way.</li><li>Also, fear of changes should be reduced, because with this approach we are creating just another type of animals, but not a cold rational machine without emotions and mercy. We are not teaching people how the robots works, but familiarity of the behavior should tell to the user‚Äôs unconscious part of the mind, that ‚Äúif something looks like a duck and behave like a duck, then it is a duck‚Äù.</li><li>At the same time, I want to restrict myself only by some degree of humaneness. Robots should be either indistinguishable of people or clearly distinguishable, to avoid of the famous effect called ‚Äúuncanny valley‚Äù where the robots are absolutely disgusting and terrifying.</li></ul><p>As the core of our brain is a so-called reptile brain, I want to start from reproducing the reptile behavior. I will return to this in several pages.</p><h2 id="basic-concept">Basic concept</h2><h3 id="program-structure">Program structure</h3><p>Trying to repeat animal‚Äôs behavior, I want to introduce such kind of robot‚Äôs program that follows the structure of human‚Äôs mind. I am not a specialist in psychiatry, so after amateur reading of C. Jung, D. Hume, E. Fromm, S. Freud, S. Harris and a lot of other great scientists who investigated the human mind, I‚Äôve decided that Jung worked with the most applicable for my purpose entities.</p><p>I am aware that science had moved much further from his time. For me, an electrical engineer, it is difficult to set apart good and reasonable modern ideas at the field of psychiatry of the bad ones. Therefore, I am relying on works of C. G. Jung as it has passed the test of time.</p><p>So, let us divide the program into parts using psychiatric concepts. Firstly, let us divide the program into two parts, the Conscious and the Unconscious. It should be more or less simple to specify what should do a Conscious is basically our Ego and will.</p><p>Next, we will use a simple idea by C. G. Jung: ‚Äúthe unconscious aggregates all psychic phenomena that do not possess quality of consciousness‚Äù.[3]</p><p>Also, Jung writes at Conscious, Unconscious, and Individuation [4] that Conscious has the only center ‚Äì Ego, while Unconscious, it seems, has no center, and represents a lot of simultaneous psychical processes. So, we will use only one dedicated thread for the Conscious and all other processes of robot‚Äôs program will be placed into multiple threads of the Unconscious.</p><p>At the same article it is claimed: ‚Äúit frequently happens that unconscious motives overrule our conscious decisions, especially in matters of vital importance‚Äù ¬†so we will implement it too, i.e. the main program representing by Conscious will be disconnectable of robot‚Äôs control in some specific cases. We will define these cases later.</p><p>Now let us define what will be defined at the Unconscious part of the robot‚Äôs mind. Obviously, it should be all processes collecting and handling information from sensors. As we never think how to match information from our two eyes, robot should not interpret information from his photosensors at the main program.</p><p>Also, Unconscious should contain all the vital parts supporting the program structure we are discussing here. So, Unconscious should have mechanisms to take control over the things in possession of Conscious and return them back. While it has the control, Unconscious should have the set of alternative programs using the same resources that the Conscious used to.</p><p>We will implement sophisticated complex behavior patterns ‚Äì instincts, as well as simple responses to external (and maybe internal) impacts ‚Äì reflexes. That is it for now. So, our program should look like:</p><ul><li>Conscious</li><li>Unconscious<ul><li>Reflexes</li><li>Instincts</li><li>Service routines</li></ul></li></ul><p>Let us determine these parts more concrete.</p><p><strong>Conscious</strong> represents simple intentions: move forward, sleep, run away, search something, etc. That part should be lightweight in terms of used resources, should consequently form simple tasks for the<strong>Unconscious.</strong></p><p><strong>Unconscious</strong> is responsible to solve how to realize those intentions: what to do for moving, how to behave in the searching process etc. Also, that part is doing all service and handling works, like pattern recognition, implement all the internal program interaction between program‚Äôs threads.</p><p><strong>Reflexes</strong> are small algorithms that can monopolize Conscious resources in some very explicit situations, e.g. emotional reactions to fear, anger, and pleasure, blinking and other automatic reaction that does not involve a lot of data handling.</p><p><strong>Instincts</strong> are also algorithms that can monopolize Conscious resources. They act a bit differently and should implements sophisticated behavior like how to behave if you are afraid, how to behave if robot is stuck somewhere (convulsions, or something), fight-or-flight response, etc.</p><p><strong>Service</strong> routines should handle information from sensors (image and sound recognition), provide the result to the<strong>Conscious, Instincts</strong> and<strong>Reflexes</strong> programs**.** Also, this part should have control over queue of the execution i.e. it should do whatever other parts are not doing.</p><p>Possible data exchange scheme is shown on Figure 1.</p><p><strong><img class="img-zoomable" src="images/Page-3-1024x595.png" alt=""/></strong></p><p><strong>Figure 1 ‚Äì Program diagram and data flows</strong></p><p>Now I will talk about reflexes.</p><h3 id="reflexes">Reflexes</h3><p><strong>A reflex</strong>, or reflex action, is an involuntary and nearly instantaneous movement in response to a stimulus. A reflex is made possible by neural pathways called reflex arcs, which can act on an impulse before that impulse reaches the brain. The reflex is then an automatic response to a stimulus that does not receive or need conscious thought. [5]</p><p>Those responses were investigated by a Russian physiologist Ivan Pavlov. And can exists in two hypostases:</p><ul><li>unconditioned response</li><li>conditioned response</li></ul><p>The last one is connected to so called Classical conditioning.</p><p><strong>Classical conditioning</strong> occurs when a conditioned stimulus (CS) is paired with an unconditioned stimulus (US). Usually, the conditioned stimulus is a neutral stimulus (e.g., the sound of a tuning fork), the unconditioned stimulus is biologically potent (e.g., the taste of food) and the unconditioned response (UR) to the unconditioned stimulus is an unlearned reflex response (e.g., salivation). After pairing is repeated the organism exhibits a conditioned response (CR) to the conditioned stimulus when the conditioned stimulus is presented alone. (A conditioned response may occur after only one pairing.) Thus, unlike the UR, the CR is acquired through experience, and it is also less permanent than the UR.<a href="https://en.wikipedia.org/wiki/Classical_conditioning" target="_blank">[6]</a></p><p>For us it is interesting because these concepts are very convenient for describing building parts of robot‚Äôs<strong>Unconscious</strong> part and also get us a hint how to implement a simple learning process without implementing neural algorithms (which are resource-intensive and non-reliable without a proper teaching process). For that problem of the simple learning I have created a separated project called<strong>r_giskard</strong> (named after the robot from Isaak Asimov‚Äôs books), but for now it is in an early state.</p><p>Now let us talk about probably one of the most important for the project part ‚Äì about emotions.</p><h3 id="emotions">Emotions</h3><p>One of the instruments that the project is using on the way to make robots more comfortable to interact with is emotions. It is unclear if the emotions are the same as reflexes<a href="https://www.iep.utm.edu/emotion/" target="_blank">[7]</a>, but it seems that emotions at least are reflexes-like. So, at this work we can handle them the same way at least at the first estimation.</p><p>While there is not one single definition of emotions, emotions have an<strong>expression</strong> component that we will use. This component can be used to expose an internal state of the robot. One of the most familiar expressions for primates are facial expressions. Facial expressions tell others what the individual tends to do in a given situation.</p><h2 id="proof-of-concept">Proof of Concept</h2><h3 id="main-idea">Main Idea</h3><p>As a first proof of the concept I‚Äôve build a robot named Zakhar (and its project is called in turn The Zakhar Project) and have prepared it for the first demo. The demonstration should expose cooperation between Conscious and Unconscious. The Conscious part of the program will execute some simple program - an infinite pattern of movement: move forward, stop, turn right, stop again move forward etc.</p><p>Then I simulate a fright. Fright program represents instinctive behavior and should make the robot express fear and make him flee until he achieves a safe place. Then at the safe place, robot is switching back to the Conscious program. Safe place is a place with much darker environment than was at the moment of the fright.</p><p>As a trigger I am using a simple change of light environment that resembles a shadow of a flying bird, e.g. flashing shadow. Because all the behavior reminds me a behavior of a small lizard (and is it the first step in my robot‚Äôs mind), I called the demonstration The Reptile Demo.</p><h3 id="criteria-of-the-success">Criteria of the Success</h3><p>As we are talking about feelings which the robot should evoke, probably we should find the hint if the concept is successful in emotional responses to the robot. Zakhar should evoke a positive or negative emotion respectfully to the situation. His actions should be clear to the observer and the observer should correctly read the context of the situation (e.g. if the robot tries to avoid something or something is attracting it). Probably it will be more obvious later, after collecting and analyzing reactions to this article and a Zakhar‚Äôs social page<a href="https://www.instagram.com/zakhar_the_robot/" target="_blank">[8]</a>.</p><h3 id="the-robots-hardware">The Robot‚Äôs hardware</h3><p>The Robot Zakhar (further, just Zakhar) consists of 4 platforms (Figure 2) connected through I2C interface:</p><ul><li>Sensor Platform</li><li>Face Platform</li><li>Computing Platform</li><li>Moving Platform</li></ul><p><img class="img-zoomable" src="images/Page-1-1024x899.png" alt=""/></p><p><strong>Figure 2 ‚Äì Zakhar the robot</strong></p><p>Moving platform represents two DC motors with rotating encoders, connected to a control board, which are in turn connected to Arduino Nano. Computing platform contains a one-board computer Raspberry Pi 3B+.</p><p>For this particular demonstration, the Sensor Platform has only one sensor ‚Äì photoresistor, which can register changes of light environment (Figure 3).</p><p><img class="img-zoomable" src="images/Zk-Page-5.png" alt=""/></p><p><strong>Figure 3 ‚Äì Light sensor</strong></p><h2 id="facial-expressions">Facial Expressions</h2><p>Right now, the project has 5 facial expressions (Figure 4). I have asked a designer<a href="https://www.instagram.com/sveta._shadow/" target="_blank">[9]</a> to draw them in a way to make expressions simple, easy to read and distinguishable.</p><p><img class="img-zoomable" src="images/Page-4.png" alt=""/></p><p><strong>Figure 4 - Faces used by the Zakhar project. Copyright (c) 2020, Svetlana Iakurnova</strong></p><p>While emotions are not implemented as separated reflexes yet, the robot‚Äôs program uses those expressions at relevant situation as part of other reflexes and instincts. These expressions are drawn onto LCD display (Figure 5)</p><p><img class="img-zoomable" src="images/6.gif" alt=""/></p><p><strong>Figure 5 ‚Äì Basic (calm) expression on the LCD with resolution of 128x64 pixels</strong></p><h2 id="in-action">In Action</h2><p>The demo consists of two parts:</p><ul><li>Patrolling</li><li>Reaction</li></ul><p>The platform for this demonstration consists of two areas (Figure 6): the shelter and the patrol area.</p><p><img class="img-zoomable" src="images/Page-2-1024x657.png" alt=""/></p><p><strong>Figure 6 ‚Äì Demo scene</strong></p><p>The starting point is the shelter. Zakhar rides out of the Shelter and starts to patrol (Figure 7).</p><p>While patrolling the layout of bird is passing above the robot. It is a trigger to turn off the Conscious and start to execute the Panic instinct. The Panic instinct is following:</p><ul><li>Stop doing anything</li><li>Express sadness</li><li>Trembling</li><li>Moving forward until the light environment falls to 0.8 of the initial light</li><li>Express calm</li><li>Trembling</li><li>Return control to the Conscious program</li></ul><p><img class="img-zoomable" src="https://media.giphy.com/media/kesVo4UVh8qzjYwyZQ/giphy.gif" alt=""/></p><p><strong>Figure 7 ‚Äì reaction to the fright</strong></p><p>At each part of the demonstration, Zakhar expresses a different emotion. Changing of the expressions with the same program is shown on Figure 8.</p><p><img class="img-zoomable" src="https://media.giphy.com/media/gKU6DKcoPdjInagmZP/giphy.gif" alt=""/></p><p><strong>Figure 8 - Expressions</strong></p><p>Videos of this demonstration are available on YouTube<a href="https://www.youtube.com/watch?v=xyb9NgWsHNY" target="_blank">[10]</a><a href="https://www.youtube.com/watch?v=Wdd7dPXa3mQ" target="_blank">[11]</a></p><h2 id="conclusion-and-further-plans">Conclusion and Further plans</h2><p>At the demonstration you can see the behavior that roughly resembles the behavior of animals. Also looking at the behavior, you can easily see and understand what the robot is trying to avoid. Instead of fear of birds, robot can possess other different phobias, like hydrophobia (for non-water-resistant robots), agoraphobia ‚Äì for robots designed to work on a small territory (like robot vacuum cleaner) and other.</p><p>On the other hand, robots can hit to use them with different kinds of philias and addictions, like addiction to charging and philias tuned to the best functional temperature, for example to keep chemical batteries longer (as you know,¬† capacities of a li-ion cell are reduced at low temperatures).</p><p>Also, I hope to make a better user experience then offered by silent and devoid of any emotion robots like Dog by Boston Dynamics and to not create terrifying uncanny robots.</p><p>At the next two iterations I am going to implement claustrophobia to keep a robot on the open spaces and mechanism of ‚Äúheat of passion‚Äù to signalize the user when his treatment of the robot is unacceptable. At the same time, I want to work on design and to develop system of measurement of user‚Äôs understanding of the robot‚Äôs behavior. The current plans and status of this experiment can be tracked on the website of the project:¬†<a href="https://zakhar.agramakov.me" target="_blank">https://zakhar.agramakov.me</a>.</p><p>Thanks for the attention!</p><p><em>Correction:<a href="https://www.kuleuven.be/wieiswie/en/person/00112699" target="_blank">Galina Deeva</a>, KU Leuven</em></p><h2 id="references">References</h2><ol><li>A Study on Technophobia and Mobile Device Design, Joong Gyu Ha, 2011</li><li>Anthropomorphism and the social robot, Brian R. Duffy, 2003</li><li>Instinct and The Unconscious, C. G. Jung, 1919</li><li>The Collected Works of C.G. Jung: Volume 9i: The Archetypes of the Collective Unconscious, 2014</li><li>Wikipedia:<a href="https://en.wikipedia.org/wiki/Reflex" target="_blank">https://en.wikipedia.org/wiki/Reflex</a></li><li>Wikipedia:<a href="https://en.wikipedia.org/wiki/Classical_conditioning" target="_blank">https://en.wikipedia.org/wiki/Classical_conditioning</a></li><li>The Internet Encyclopedia of Philosophy:<a href="https://www.iep.utm.edu/emotion/" target="_blank">https://www.iep.utm.edu/emotion/</a></li><li>Instagram:<a href="https://www.instagram.com/zakhar_the_robot/" target="_blank">https://www.instagram.com/zakhar_the_robot/</a></li><li>Svetlana Iakurnova:¬†<a href="https://www.behance.net/svetlanayakurnova" target="_blank">Behance.net/svetlanayakurnova;</a>¬†<a href="https://www.instagram.com/sveta._shadow/" target="_blank">Instagram.com/sveta._shadow/</a></li><li>YouTube:<a href="https://www.youtube.com/watch?v=xyb9NgWsHNY" target="_blank">https://www.youtube.com/watch?v=xyb9NgWsHNY</a></li><li>YouTube:<a href="https://www.youtube.com/watch?v=Wdd7dPXa3mQ" target="_blank">https://www.youtube.com/watch?v=Wdd7dPXa3mQ</a></li></ol><h2 id="links">Links</h2><ul><li><strong><a href="https://zakhar.agramakov.me/" target="_blank">Project&rsquo;s site</a></strong></li><li><strong><a href="https://hackaday.io/project/171888-zakhar-the-robot" target="_blank">Project news on hackaday.io</a></strong> and<strong><a href="https://blog.agramakov.me/category/zakhar/" target="_blank">blog on the dev&rsquo;s site</a></strong></li><li><strong><a href="https://www.instagram.com/zakhar_the_robot/" target="_blank">Instagram</a></strong></li><li><strong><a href="https://github.com/an-dr/zakhar" target="_blank">Sources on GitHub</a></strong></li><li><strong><a href="https://www.hackster.io/an-dr/zakhar-the-robot-0d8744" target="_blank">Build instruction on hackster.io</a></strong></li></ul><p>Developer: Andrei Gramakov -¬†<strong><a href="https://agramakov.me/" target="_blank">agramakov.me</a></strong></p></description></item><item><title>Moving to the Robot Operating System</title><link>https://blog.agramakov.me/posts/2020/05-27-moving-to-the-robot-operating-system/</link><pubDate>Wed, 27 May 2020 22:57:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-27-moving-to-the-robot-operating-system/</guid><description><p>The first experiments with ROS showed that it probably will help a lot in building mind-like organized programs.</p><p>Node-based conception and server-client interactions hopefully allow me to think less about queues, threads, and access control, which took the most of the time during building the first demo on bare Python.</p><p>Here are some ideas of the mind-like program architecture. I probably will working on it next weekend.</p><p><img class="img-zoomable" src="4773851590620014679.png" alt=""/>
Colored blocks are nodes</p></description></item><item><title>The Reptile Demo</title><link>https://blog.agramakov.me/posts/2020/05-25-the-reptile-demo/</link><pubDate>Mon, 25 May 2020 15:17:00 +0200</pubDate><guid>https://blog.agramakov.me/posts/2020/05-25-the-reptile-demo/</guid><description><p>While I&rsquo;m finishing a new article about the project, please look the video demonstration. Without the context for now</p><div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"><iframe src="https://www.youtube.com/embed/xyb9NgWsHNY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen= title="YouTube Video"/></div></description></item><item><title>Zakhar's Concept</title><link>https://blog.agramakov.me/posts/2019/06-01-zakhars-concept/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.agramakov.me/posts/2019/06-01-zakhars-concept/</guid><description><p>It is not a secret for anybody, what we are living in the era of consumption. We produce and consume food, medical preparations and of course, the flagship of the planet&rsquo;s economics - electronics. Modern electronics are becoming more and more sophisticated, hard to understand and we have to spend time to learn it. It so, because there is a huge gap between the level of development of technics (which the main face is electronics) and user interfaces. &ldquo;User interfaces&rdquo; it is a quite new term, and researches in that fields are conducted only in the last several decades. Previously we could talk about ergonomic, but the question of how to use a technical tool was not asked hard way as now. Let&rsquo;s take a look at the most technologically advanced and widespread instrument, we have today - a smartphone. Talking about people, using it in most of the context we have to split them into &ldquo;users&rdquo; and as-called &ldquo;power users&rdquo;. The first category is the people, who are using only basic functions of the device. They can write a message, make a call, charge a device and take a photo. Commonly they are do not understand the behavior of the device, they do not know if the device could work under the water, at the cold temperatures, they are do not know how say, Android devices will cooperate with each other is touch it&rsquo;s backsides or how to transfer text from an iPhone to a MacBook. It is a really natural and normal situation, cause a man is a social creature - that is mean, the most important skills it has, how to cooperate with other people. Talking about interacting with others a middle man is a power user. That is so because of those points:</p><ol><li>To understand others is the vital skill for any highly organized animals</li><li>To understand the self is the vital skill for any form of life</li><li>Thanks the similarity of any human, the first and the second quality is connected.</li></ol><p>Because of these circumstances, we can understand each other, we can interact with each other productively, became a power user of each other. The more similarity between us and other animals, cause better cooperation. Look, how we interacting with dogs or cows, look at how we interact with jellyfish and bees. I claim, the more resembling in inner processes, especially that connected with the outer behavior tightly, the more effectiveness and simplicity we are getting during interacting. Talking about such artificial thing the technics are, we are not understanding how it works, we can not imagine how any our interact connected with the result which technics provide are - without preparations and learning of the subject. There is a lot of approaches on how to harmonize interfaces with our perception, but making systems complicated all the time requiring finding new approaches. At modern times we can talk to our smartphones, we can use unmanned transport, we are almost got robots as companions. But in many cases, without learning rules and conventions of that kind interacting of us with our technical tools is useless. I can see two main ways how to simplify interacting between users and a technics:</p><ul><li>Learn how to a technics works</li><li>Learn a technics to work like other people</li></ul><p>The first approach we are using all our modern lives, the second one became more important in the last years (Google Duplex is an ambassador of this approach). As I said we are standing on the start line of the mass robotics. But still and all the time when we are talking about it (are do that a lot!) we are considering, what the robots would be fundamentally different from life existing before. We are justifying this based on facts what electronics based on different principles than organics. And we are ready to create sophisticated robots with a straight fully logical program, which can not understand us and therefore could lead us to different problems including a total mankind extinction. We are trying to develop systems to control and to hold AI, we are afraid of the future so much what we can not see the most obvious thing. We already can control and understand each other perfectly. Evidence of this is our existing for thousands of years. I suppose, if we really want (directly or indirectly) to create a new form of life which could co-exist with us there is the one way - study yourself and make something based on that we are now is. In my robotics hobby, I‚Äôm trying to follow ideas which are at the basis of the human mind. And for it is a more interesting approach than just reproduce elements of Rumba in my robots. I was thinking a lot about how the human mind works. Fortunately almost all human culture, starting from literature ending the serials - is a clue to an understanding of this. I can tell at least 4 relatively solid and standalone parts of the human mind:</p><ol><li>Conscious - the<a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/2001-02/hci/locus.htm" target="_blank">locus of attention</a>, watcher of the current process.</li><li>Subconscious - the part of the mind where the most sophisticated thing is doing. Subconscious is learning all the time, it collects and full the input data by meaning. All the time we learning any new move in dance or yoga, subconscious being under the conscious watch trying to figure out how to connect out desire with body‚Äôs capabilities.</li><li>Unconscious - represents our reflexes and instincts. When something goes out the control, the unconscious is turning off a conscious and use subconscious with all it‚Äôs skills to follow things to a more appropriate way, say to take away a hand out a fire. It is the source of the fight-or-flight response, reaction for the pain and so on.</li><li>Autonomous systems. Not all parts of our bodies could be following by direct orders of conscience or unconscious. The heart will works until you died, the hair will grow even further and we can not change it significantly by own.</li></ol><p>At<a href="https://blog.agramakov.me/2019/05/05/zakhar-relaunch-zakha_ros/" target="_blank">my previous article</a>, I told my main robotics project - Zakhar the Robot is relaunched to get a new name,<code>Zakha_ros</code>. And following there is an idea of how it should work keeping in mind all the said previously:</p><p><img class="img-zoomable" src="za_concept.png" alt=""/></p><p>In terms of the software, Unconscious seems like the kind of actual human operating system. It could start, pause or terminate the Conscious. It is a stable core which is changing rarely and being a standalone part of our minds. Conscious - is the child process of Unconscious and the second user of the Subconscious. Talking about manned robots - it is an operator using human-computer interfaces. Talking about unmanned ones - who knows. Subconscious - is the most flexible part of the mind. This is it‚Äôs the main mass and instrument. For robots - the most logic and hardware abstraction layer concluded here. It is a rewritable program, which is updating all the time. But as a first approximation, we could consider it as a fixed one. Autonomy systems - the simplest part. It could be any device, controlled or not with other parts of a robot. But mandatory connected with the robot. Following these ideas and developing it‚Äôs the next step I‚Äôm planning to realize a moving system of<code>Zakha_ros</code>, so stay tuned!</p></description></item><item><title>Zakhar relaunch: Zakha_ros</title><link>https://blog.agramakov.me/posts/2019/05-05-zakhar-relaunch-zakha_ros/</link><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid>https://blog.agramakov.me/posts/2019/05-05-zakhar-relaunch-zakha_ros/</guid><description><p>After a lot of experiments with Raspberry, ESP32, STM32 and Arduino boards I&rsquo;ve decided what will be a spine of the<a href="https://github.com/an-dr/zakhar" target="_blank">Zakhar</a> project.<a href="https://www.ros.org/" target="_blank">ROS</a> seems the most interesting and extendible. Inside it, I could combine Python, C, C++ programs (hence, libraries ) working inside a Linux environment and communicate with any other platforms.</p><p>Say, I could build a system on a Raspberry, which will communicate with developer board like<a href="https://www.espressif.com/en/products/hardware/esp-wrover-kit/overview" target="_blank">ESP-WROVER-KIT</a> working on FreeRTOS. Even more, I could use my desktop as a part of the system for High-level computing (say, for image recognition with OpenCV).</p><p>I have bought some hardware for the new version of Zakhar, called<strong>Zakha_ros:</strong></p><h2 id="the-new-hardware">The new hardware</h2><p><strong>1. Platform.</strong> It is a simple platform which I used for my first experiments 3 years ago:</p><p><img class="img-zoomable" src="za_platform_logo.jpg" alt=""/></p><p><strong>2. Manipulator.</strong> It based on DC motor and an aluminum chassis. Nice to grab something.</p><p><img class="img-zoomable" src="za_hand_logo.jpg" alt=""/></p><p><strong>3. Camera.</strong> I&rsquo;ve bought a standard Pi camera than mounted it onto a small cardboard panel</p><p><img class="img-zoomable" src="za_camera_logo.jpg" alt=""/></p><p><strong>4. Raspberry Pi 3 B+.</strong> With a nice black case.</p><p><img class="img-zoomable" src="za_rpi_logo.jpg" alt=""/></p><p>¬†</p><h2 id="whats-next">What&rsquo;s Next</h2><p>Next, I&rsquo;m planning to update a<a href="https://agramakov.me/wiki/doku.php?id=zakhar:main" target="_blank">Zakhar concept</a> to follow ROS capabilities and using hardware. Stay tuned!</p></description></item></channel></rss>